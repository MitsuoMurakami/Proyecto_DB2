{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f72575afc9b54cf98a7b62a31329ed15",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "id": "fr2ewyCfSkO1"
   },
   "source": [
    "# SPIMI Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4b61a710a23643a695c0c0082646f4cf",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### 1- Estructura del índice invertido en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:52:39.611088Z",
     "start_time": "2024-10-24T09:52:39.607071Z"
    },
    "cell_id": "c2d154f448dc41d8b348eb9d9c1b48bf",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nindex = {\\nw1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\\nw2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\\nw3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\\n}\\n\\nidf = {\\nw1 : idf_w1,\\nw2 : idf_w2,\\nw3 : idf_w3,\\n}\\n\\nlength ={\\ndoc1: norm_doc1,\\ndoc2: norm_doc2,\\ndoc3: norm_doc3,\\n...\\n}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "El indice esta estructurado de la siguiente forma, contiene el idf para cada termino y su posting list. \n",
    "La posting list contiene el id del documento y la frecuencia del termino en el documento.\n",
    "\n",
    "index.txt :\n",
    "\n",
    "w1 : idf_w1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\n",
    "w2 : idf_w2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\n",
    "w3 : idf_w3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\n",
    "\n",
    "position_terms.pkl:\n",
    "\n",
    "length ={\n",
    "doc1: norm_doc1,\n",
    "doc2: norm_doc2,\n",
    "doc3: norm_doc3,\n",
    "...\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ccacf43c13742dbaf088ceb45b5c621",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# SPIMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:25:00.257203Z",
     "start_time": "2024-11-15T22:25:00.235674Z"
    },
    "cell_id": "dca6b48d30794b3e86f5da878ce5451b",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "1e7e5f1c-d694-4140-b3f8-562293cc6b23",
    "execution_millis": 0,
    "execution_start": 1728075206132,
    "source_hash": "d90d743f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import os\n",
    "import sys\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class InvertIndexSPIMI:\n",
    "    def __init__(self):\n",
    "      \n",
    "        self.index_main = {} # dictionary to store the position of each term in the index file\n",
    "        self.length_main = {} # dictionary to store the norm for each document\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stoplist = []\n",
    "        self.preprocessed_words_file = \"preprocessed_words.txt\"\n",
    "        self.spimi_file = \"spimi_blocks.txt\"\n",
    "        self.id_block = 0\n",
    "        self.free_memory_available = 500000  # 0.5 MB\n",
    "        self.current_line_preprocessed_words_file = 0\n",
    "        self.end_preprocessed_words_file = 0\n",
    "        self.min_heap_size = 500\n",
    "        self.index = \"index.txt\"\n",
    "        self.cant_documents = 0\n",
    "        self.norma_docs_file = \"norma_docs.pkl\" \n",
    "        self.position_terms_file = \"position_terms.pkl\"\n",
    "        self.original_file = \"\"\n",
    "        #fd\n",
    " # TODO Process the text to create many blocks using the SPIMI algorithm\n",
    "    def load_stop_list(self, filename=\"english_stoplist.txt\"):\n",
    "        with open(filename, encoding=\"utf-8\") as file:\n",
    "            self.stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "    def preprocesamiento_aux(self, texto):\n",
    "        words = []\n",
    "        # tokenizar\n",
    "        texto_tok = texto.lower()\n",
    "        texto_tok = re.sub(r'[^a-zA-Z_À-ÿ]', ' ', texto)\n",
    "        words = nltk.word_tokenize(texto_tok, language='english')\n",
    "        # filtrar stopwords\n",
    "        words = [word for word in words if word not in self.stoplist]\n",
    "        # reducir palabras\n",
    "        words = [self.stemmer.stem(word) for word in words]\n",
    "        return words\n",
    "    def get_list_tf(self, tokens, id):\n",
    "        # compute the term frequency for each word in the document with id\n",
    "        tf = {}\n",
    "        for word in tokens:\n",
    "            if word not in tf:\n",
    "                tf[word] = 1\n",
    "            else:\n",
    "                tf[word] += 1\n",
    "        return [(id, word, tf[word]) for word in tf]\n",
    "    # process the text to create the preprocessed words file\n",
    "    def preprocesamiento(self, pathOfFile):\n",
    "        print(\"Preprocessing songs\")      \n",
    "        self.original_file = pathOfFile\n",
    "        # List of tokens for each song\n",
    "        preprocessed_song = []           \n",
    "        with open(pathOfFile, newline='', encoding='utf-8') as file, open(self.preprocessed_words_file, 'w') as preprocessed_words:\n",
    "            songs_file = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "            # counter used as id for each song\n",
    "            counter_rows = 0\n",
    "            for row in songs_file:\n",
    "                # skip the header\n",
    "                if counter_rows == 0:\n",
    "                    counter_rows += 1\n",
    "                    continue\n",
    "                # track_name , artist_name, lyrics\n",
    "                song_details = row[1] + \" \" + row[2] + \" \" + row[3]\n",
    "                # get list of tokens for each song\n",
    "                preprocessed_song = self.preprocesamiento_aux(song_details)\n",
    "                # get the tf for each word in the song\n",
    "                list_tf_song = self.get_list_tf(preprocessed_song, counter_rows)\n",
    "                counter_rows += 1\n",
    "                # write the relative_id_song, word, tf in each line of file\n",
    "                for tf in list_tf_song:\n",
    "                    preprocessed_words.write(str(tf) + \"\\n\")\n",
    "            self.cant_documents = counter_rows\n",
    "          \n",
    "    def strToList(self, string):\n",
    "        string = string[:-1] \n",
    "        string = string[1:-1]\n",
    "        tup = string.split(\", \")\n",
    "        return [int(tup[0]), tup[1], int(tup[2])]\n",
    "    \n",
    "    # receive the token list which is a list of tuples (id, word, tf)\n",
    "    def spimi_invert(self):\n",
    "        blocks_directory = r\".\\blocks\"\n",
    "        if not os.path.exists(blocks_directory):\n",
    "            os.makedirs(blocks_directory)\n",
    "        output_file = r\".\\blocks\\block_\" + str(self.id_block) + \".txt\"\n",
    "        dictionary = {}\n",
    "        with open(self.preprocessed_words_file, 'r') as preprocessed_words, open(output_file, 'w') as block:\n",
    "            print(\"In block:\", self.id_block)\n",
    "            preprocessed_words.seek(self.current_line_preprocessed_words_file)\n",
    "            while sys.getsizeof(dictionary) < self.free_memory_available:\n",
    "                # read the next token\n",
    "                token = preprocessed_words.readline()\n",
    "                if not token:\n",
    "                    self.end_preprocessed_words_file = 1\n",
    "                    break\n",
    "                # convert the string to a list of relative_id_song, word and its tf\n",
    "                token = self.strToList(token)\n",
    "                if token[1] not in dictionary:\n",
    "                    dictionary[token[1]] = [(token[0], token[2])]\n",
    "                else:\n",
    "                    dictionary[token[1]].append((token[0], token[2]))\n",
    "            self.current_line_preprocessed_words_file = preprocessed_words.tell()\n",
    "            sorted_terms = sorted(dictionary.keys())\n",
    "            for term in sorted_terms:\n",
    "                block.write(term + \":\" + str(dictionary[term]) + \"\\n\")\n",
    "        self.id_block += 1\n",
    "        \n",
    "        \n",
    "    def create_spimi_blocks(self): \n",
    "        print(\"Creating SPIMI blocks\")\n",
    "        # create a directory to store the blocks\n",
    "        blocks_directory = r\".\\blocks\"\n",
    "        if not os.path.exists(blocks_directory):\n",
    "            os.makedirs(blocks_directory)\n",
    "        # read the preprocessed words file as it has the tf for each token in each song, then created size \n",
    "        # limit blocks with spimi algorithm \n",
    "        while(not self.end_preprocessed_words_file):  \n",
    "            self.spimi_invert()\n",
    "       \n",
    "   \n",
    "    \"\"\"\n",
    "    Para el merge se abriran todos los bloques simultaneamente, se leera la primera palabras de cada bloque,\n",
    "    luego usando una pritority queue se obtendra el menor termino lexicografico y se combinaran todas las \n",
    "    listas existentes para dicho termino. Se escribira en un nuevo archivo y se repetira el proceso hasta \n",
    "    que no haya mas.\n",
    "    Dado que se el minHeap se inicializa con 1 termino de cada bloque y cada vez que se hace pop, se pushea \n",
    "    un term del mismo bloque, se garantiza que el heap tiene un term de cada bloque a menos que se haya llegado\n",
    "    al final de un bloque. Ademas, se itera hasta que el heap este vacio, por lo que se garantiza el \n",
    "    recorrido total por todos los bloques.\n",
    "    \"\"\"\n",
    "   \n",
    "    def merge_spimi_blocks(self):\n",
    "        print(\"Merging SPIMI blocks\")\n",
    "        # get all the block names\n",
    "        blocks_name = os.listdir(r\".\\blocks\")\n",
    "        # open all the blocks\n",
    "        opened_blocks = []\n",
    "        for block_name in blocks_name:\n",
    "            opened_blocks.append(open(\".\\\\blocks\\\\\" + block_name, 'r'))\n",
    "        # min heap for terms\n",
    "        terms_from_all_blocks = []\n",
    "        # dictionary to store the norm for each document\n",
    "        temp_norm_dict = {}\n",
    "        # dictionary to store the position of each term in the index file\n",
    "        position_terms = {}\n",
    "        # read the first term from each block\n",
    "        counter_opened_block = 0\n",
    "        for opened_block in opened_blocks:\n",
    "            term = opened_block.readline()\n",
    "            if term:\n",
    "                term = term.split(\":\")\n",
    "                # push (term, [...], block_number) to the heap\n",
    "                heapq.heappush(terms_from_all_blocks, (term[0], term[1], counter_opened_block))\n",
    "            counter_opened_block += 1\n",
    "        # merge the blocks in index.txt\n",
    "        with open(self.index, 'w') as index:\n",
    "            prev_top_term = None\n",
    "            temp_prev_top_term = []\n",
    "            \n",
    "            # iterate until the heap is empty\n",
    "            while len(terms_from_all_blocks) > 0:\n",
    "                # obtenemos el menor termino lexicograficamente \n",
    "                current_top_term = heapq.heappop(terms_from_all_blocks)\n",
    "                # if the previous term is the same as the current term, merge their posting lists\n",
    "                if prev_top_term and prev_top_term[0] == current_top_term[0]:\n",
    "                    # convert the string to a list of tuples if necessary\n",
    "                    if isinstance(prev_top_term[1], str):                \n",
    "                        temp_prev_top_term = eval(prev_top_term[1])\n",
    "                    # posting list of current term is always a str\n",
    "                    temp_current_top_term = eval(current_top_term[1])\n",
    "                    # merge posting lists\n",
    "                    temp_prev_top_term.extend(temp_current_top_term)\n",
    "                    # update the posting list and block number \n",
    "                    prev_top_term =(prev_top_term[0], temp_prev_top_term, current_top_term[2])\n",
    "                    \n",
    "                else:\n",
    "                    prev_top_term = current_top_term\n",
    "                    # transform to List if posting list is a str\n",
    "                    if isinstance(current_top_term[1], str):\n",
    "                        temp_posting_list = eval(current_top_term[1])\n",
    "                        current_top_term = (current_top_term[0], temp_posting_list, current_top_term[2])\n",
    "                    # compute the idf\n",
    "                    idf_t = np.log10(self.cant_documents / len(current_top_term[1]))\n",
    "                    # compute the tf_idf squared weights of each term for each document, doc is a tuple (doc_id, tf)\n",
    "                    for doc in current_top_term[1]:\n",
    "                        if doc[0] not in temp_norm_dict:\n",
    "                            temp_norm_dict[doc[0]] = ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                        else:\n",
    "                            temp_norm_dict[doc[0]] += ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                    \n",
    "                    # store the position of the term in the index file\n",
    "                    position_terms[current_top_term[0][1:-1]] = index.tell()\n",
    "                    # write the term to the index -> term:idf:posting_list \n",
    "                    index.write(current_top_term[0] + \":\" + str(idf_t) + \":\" + str(current_top_term[1]) + \"\\n\")\n",
    "                    \n",
    "                # read the next term from the block of the current term\n",
    "                next_term_same_block = opened_blocks[current_top_term[2]].readline()\n",
    "                if next_term_same_block:\n",
    "                       next_term_same_block = next_term_same_block.split(\":\")\n",
    "                       # push the next term to the heap\n",
    "                       heapq.heappush(terms_from_all_blocks, (next_term_same_block[0], next_term_same_block[1], current_top_term[2]))\n",
    "        \n",
    "        # compute the norm of each document\n",
    "        for doc in temp_norm_dict:\n",
    "            temp_norm_dict[doc] = np.sqrt(temp_norm_dict[doc])\n",
    "        print(\"Blocks merged\")\n",
    "        # write norms to disk with pickle to recover them later in dictionary format\n",
    "        with open(self.norma_docs_file, 'wb') as file:\n",
    "            pickle.dump(temp_norm_dict, file)\n",
    "        # write the position of each term in the index file to disk with pickle to recover them later\n",
    "        with open(self.position_terms_file, 'wb') as file:\n",
    "            pickle.dump(position_terms, file)\n",
    "        # close blocks\n",
    "        for opened_block in opened_blocks:\n",
    "            opened_block.close()\n",
    "        print(\"Index created\")  \n",
    "    # build the index\n",
    "    def build (self, pathOfFile):\n",
    "        # FIXME: Only build the process if the index file does not exist for development purposes\n",
    "        if os.path.exists(self.index):\n",
    "            return\n",
    "        self.load_stop_list()\n",
    "        self.preprocesamiento(pathOfFile)\n",
    "        self.create_spimi_blocks()\n",
    "        self.merge_spimi_blocks()\n",
    "    \n",
    "    def load_index(self):\n",
    "        # load index (position of terms in index file) and the norm of documents from disk       \n",
    "        try:\n",
    "            with open(self.position_terms_file, 'rb') as file:\n",
    "                self.index_main = pickle.load(file)\n",
    "                print(\"Index main ready\")\n",
    "            with open(self.norma_docs_file, 'rb') as file:\n",
    "                self.length_main = pickle.load(file)\n",
    "                print(\"Length main ready\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Index not found\")\n",
    "\n",
    "        \n",
    "    def retrieval(self, query, k):\n",
    "        # load pos terms and norm docs\n",
    "        self.load_index()\n",
    "        # diccionario para el score\n",
    "        score = {}\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        post_query = self.preprocesamiento_aux(query)\n",
    "        # calcular el tf-idf del query\n",
    "        query_words = set(post_query)\n",
    "        query_tfidf = []\n",
    "        with open(self.index, 'r') as idx_file:\n",
    "            for wordq in query_words:\n",
    "                idfq = 0\n",
    "                if wordq in self.index_main:\n",
    "                    # posicion del termino en el index \n",
    "                    pos_wordq = self.index_main[wordq]\n",
    "                    # read the term from the file\n",
    "                    idx_file.seek(pos_wordq)\n",
    "                    term_info = idx_file.readline().split(\":\")\n",
    "                    # idf del termino\n",
    "                    idfq = float(term_info[1])\n",
    "              \n",
    "                  \n",
    "                # tf.idf del termino en el query \n",
    "                query_tfidf.append((1 + np.log10(post_query.count(wordq))) * idfq)\n",
    "        # compute the norm of the query \n",
    "       \n",
    "        query_norm = np.sqrt(sum([q ** 2 for q in query_tfidf]))\n",
    "        # normalizar la query\n",
    "       \n",
    "        query_tfidf = [q / query_norm for q in query_tfidf]\n",
    "        query_words = list(query_words)\n",
    "\n",
    "\n",
    "        temp__ = {}\n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "        with open(self.index, 'r') as idx_file:\n",
    "            for word in query_words:\n",
    "                if word in self.index_main:\n",
    "                    pos_word = self.index_main[word]\n",
    "                    idx_file.seek(pos_word)\n",
    "                    term_info = idx_file.readline().split(\":\")\n",
    "                    idf = float(term_info[1])\n",
    "                    posting_list_ = eval(term_info[2])\n",
    "               \n",
    "                    for doc, tf in posting_list_:\n",
    "                        if doc not in score:\n",
    "                            score[doc] = 0\n",
    "                            temp__[doc] = 0\n",
    "                        # tf.idf del termino en el documento por el tf.idf del termino en el query\n",
    "                        tf_idf_ = (1 + np.log10(tf)) * idf\n",
    "                        score[doc] += query_tfidf[query_words.index(word)] * tf_idf_\n",
    "                        temp__[doc] +=(tf*idf)\n",
    "            \n",
    "           \n",
    "            for d in score:\n",
    "                # normalizar el score\n",
    "                score[d] = score[d] / self.length_main[d]\n",
    "                temp__[d] = temp__[d] / self.length_main[d]\n",
    "        \n",
    "        # transform the dictionary to a list of tuples and sort it by the score \n",
    "        result = sorted(score.items(), key=lambda tup: tup[1], reverse=True) \n",
    "        # return only the keys\n",
    "        # result = [key for key, value in result]\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        print(\"Results retrieved\")\n",
    "        print(\"Top\", k, \"songs:\")\n",
    "        return result[:k]\n",
    "    # FIXME 2: display the results in a dataframe\n",
    "    def displayResults(self, result):\n",
    "       \n",
    "        track_ids = []\n",
    "        track_name = []\n",
    "        artists = []\n",
    "        lyrics = []\n",
    "        scores = []\n",
    "        \n",
    "        # open the file with the songs\n",
    "        songs = pd.read_csv(self.original_file, delimiter=',', quotechar='\"')\n",
    "        for item in result:\n",
    "            # info_new = \"Rank:\" + str(i)+ \"Url :\"+ dataton[\"url\"][resuldato[0]]+ \"score: \"+ str(resuldato[1])\n",
    "            #result_news.append(info_new)\n",
    "            \n",
    "            song = songs.iloc[item[0]- 1] \n",
    "            track_ids.append(song['track_id'])\n",
    "            track_name.append(song['track_name'])\n",
    "            artists.append(song['track_artist'])\n",
    "            lyrics.append(song['lyrics'])\n",
    "            scores.append(str(item[1]))\n",
    "       \n",
    "        df_news = pd.DataFrame({\"ID\": track_ids, \"Track Name\": track_name,\"Artist\": artists ,\"Lyrics\": lyrics, \"Score\": scores})\n",
    "        return df_news\n",
    "\n",
    " # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:26:08.365662Z",
     "start_time": "2024-11-15T22:25:06.205568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing songs\n",
      "Creating SPIMI blocks\n",
      "In block: 0\n",
      "In block: 1\n",
      "In block: 2\n",
      "Merging SPIMI blocks\n",
      "Blocks merged\n",
      "Index created\n"
     ]
    }
   ],
   "source": [
    "# Test preprocess songs\n",
    "index = InvertIndexSPIMI()\n",
    "index.build(r\".\\Postgres\\spotify_songs2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:27:15.096605Z",
     "start_time": "2024-11-15T22:27:15.070170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Query tfidf mas: 1.0\n",
      "Results retrieved\n",
      "Top 5 songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(827, np.float64(1.0)),\n",
       " (3118, np.float64(0.9735013010153005)),\n",
       " (1184, np.float64(0.9183348199849688)),\n",
       " (3370, np.float64(0.9050890673717692)),\n",
       " (785, np.float64(0.8583978883774164))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test retrieval\n",
    "# TODO: try gradio, display the results in a dataframe\n",
    "# TODO: integrate with the gui of postgres\n",
    "# TODO: delete comments, clean\n",
    "index.retrieval(\"love\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:27:24.558527Z",
     "start_time": "2024-11-15T22:27:24.170332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Query tfidf mas: 1.0\n",
      "Results retrieved\n",
      "Top 10 songs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ryUbYQzdRRw0iJ2P5Or55</td>\n",
       "      <td>Funky Ride</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Let me take you on a funky ride All around the...</td>\n",
       "      <td>0.18812117562945896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7xkt4VVkUfC7MLtb2axxiC</td>\n",
       "      <td>Hey Up There (feat. Ty Dolla $ign)</td>\n",
       "      <td>Buddy</td>\n",
       "      <td>Hey up there, I'm on my way up Tell the radio ...</td>\n",
       "      <td>0.12389941978016338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6gEejYQZRMeMODb5rxwdK8</td>\n",
       "      <td>Myintrotoletuknow</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Time and time again see I be thinking about th...</td>\n",
       "      <td>0.12210220578149839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7nfGrjZf34FxqS0FWXM5B7</td>\n",
       "      <td>Wailin'</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>In the zone like Keyser Söze, always the Usual...</td>\n",
       "      <td>0.10369982450419457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5qKqRkWnRTXNVWbEOOmGxk</td>\n",
       "      <td>Wheelz of Steel</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Uh, as I sit in my b-boy stance With flip-flop...</td>\n",
       "      <td>0.097631487356621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5voPrNQYQ4kQoRZ9A2NghB</td>\n",
       "      <td>Bowtie (feat. Sleepy Brown &amp; Jazze Pha)</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Girl you cut up (Lucius Left Foot) Girl you kn...</td>\n",
       "      <td>0.0926095476092194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5yTQ7Vy6EwKwB64Vho90LI</td>\n",
       "      <td>West Savannah</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Yeah, I'm back off in this bitch one more time...</td>\n",
       "      <td>0.08975331579503788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62NQ8JoW3yfJ2WSmlnaogw</td>\n",
       "      <td>Born In The Trap</td>\n",
       "      <td>The Game</td>\n",
       "      <td>I was born in the crosshairs without a pot to ...</td>\n",
       "      <td>0.08587607674530255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6bUNEbXT7HovLW6BgPCBsb</td>\n",
       "      <td>Roses</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Caroline (Caroline), see, Caroline All the guy...</td>\n",
       "      <td>0.07874564466918879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6gQvUs5abDFspk9LAlCfKa</td>\n",
       "      <td>Thought Process</td>\n",
       "      <td>Goodie Mob</td>\n",
       "      <td>Let me get a chop at this lumber Niggas from d...</td>\n",
       "      <td>0.07701821742684782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                               Track Name  \\\n",
       "0  5ryUbYQzdRRw0iJ2P5Or55                               Funky Ride   \n",
       "1  7xkt4VVkUfC7MLtb2axxiC       Hey Up There (feat. Ty Dolla $ign)   \n",
       "2  6gEejYQZRMeMODb5rxwdK8                        Myintrotoletuknow   \n",
       "3  7nfGrjZf34FxqS0FWXM5B7                                  Wailin'   \n",
       "4  5qKqRkWnRTXNVWbEOOmGxk                          Wheelz of Steel   \n",
       "5  5voPrNQYQ4kQoRZ9A2NghB  Bowtie (feat. Sleepy Brown & Jazze Pha)   \n",
       "6  5yTQ7Vy6EwKwB64Vho90LI                            West Savannah   \n",
       "7  62NQ8JoW3yfJ2WSmlnaogw                         Born In The Trap   \n",
       "8  6bUNEbXT7HovLW6BgPCBsb                                    Roses   \n",
       "9  6gQvUs5abDFspk9LAlCfKa                          Thought Process   \n",
       "\n",
       "       Artist                                             Lyrics  \\\n",
       "0     OutKast  Let me take you on a funky ride All around the...   \n",
       "1       Buddy  Hey up there, I'm on my way up Tell the radio ...   \n",
       "2     OutKast  Time and time again see I be thinking about th...   \n",
       "3     OutKast  In the zone like Keyser Söze, always the Usual...   \n",
       "4     OutKast  Uh, as I sit in my b-boy stance With flip-flop...   \n",
       "5     OutKast  Girl you cut up (Lucius Left Foot) Girl you kn...   \n",
       "6     OutKast  Yeah, I'm back off in this bitch one more time...   \n",
       "7    The Game  I was born in the crosshairs without a pot to ...   \n",
       "8     OutKast  Caroline (Caroline), see, Caroline All the guy...   \n",
       "9  Goodie Mob  Let me get a chop at this lumber Niggas from d...   \n",
       "\n",
       "                 Score  \n",
       "0  0.18812117562945896  \n",
       "1  0.12389941978016338  \n",
       "2  0.12210220578149839  \n",
       "3  0.10369982450419457  \n",
       "4    0.097631487356621  \n",
       "5   0.0926095476092194  \n",
       "6  0.08975331579503788  \n",
       "7  0.08587607674530255  \n",
       "8  0.07874564466918879  \n",
       "9  0.07701821742684782  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.displayResults(index.retrieval(\"Outkast\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:04:31.347361Z",
     "start_time": "2024-11-15T23:04:30.956670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id                                         track_name  \\\n",
      "0  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
      "1  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
      "2  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
      "3  00emjlCv9azBN0fzuuyLqy                                         Dumb Litty   \n",
      "4  00f9VGHfQhAHMCQ2bSjg3D                                            Soldier   \n",
      "\n",
      "     track_artist                                             lyrics  \\\n",
      "0   Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
      "1  Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
      "2     CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
      "3            KARD  Get up out of my business You don't keep me fr...   \n",
      "4        James TW  Hold your breath, don't look down, keep trying...   \n",
      "\n",
      "   track_popularity          track_album_id  \\\n",
      "0                28  3z04Lb9Dsilqw68SHt6jLB   \n",
      "1                 0  6oZ6brjB8x3GoeSYdwJdPc   \n",
      "2                41  3ssspRe42CXkhPxdc12xcp   \n",
      "3                65  7h5X3xhh3peIK9Y0qI5hbK   \n",
      "4                70  3GNzXsFbzdwM0WKCZtgeNP   \n",
      "\n",
      "                       track_album_name track_album_release_date  \\\n",
      "0                           Love & Loss               2017-11-21   \n",
      "1                                  Gold               2005-01-01   \n",
      "2                  CeeLo's Magic Moment               2012-10-29   \n",
      "3  KARD 2nd Digital Single ‘Dumb Litty’               2019-09-22   \n",
      "4                              Chapters               2019-04-26   \n",
      "\n",
      "                                       playlist_name             playlist_id  \\\n",
      "0                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
      "1  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
      "2                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
      "3                                  K-Party Dance Mix  37i9dQZF1DX4RDXswvP6Mj   \n",
      "4                                 urban contemporary  4WiB26kw0INKwbzfb5M6Tv   \n",
      "\n",
      "   ... loudness mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
      "0  ...   -4.739    1       0.0442       0.01170           0.00994    0.3470   \n",
      "1  ...   -7.504    0       0.2160       0.00432           0.00723    0.4890   \n",
      "2  ...   -5.819    0       0.0341       0.68900           0.00000    0.0664   \n",
      "3  ...   -1.993    1       0.0409       0.03700           0.00000    0.1380   \n",
      "4  ...   -6.157    1       0.0550       0.28000           0.00000    0.0975   \n",
      "\n",
      "   valence    tempo  duration_ms  language  \n",
      "0    0.404  135.225       373512        en  \n",
      "1    0.650  111.904       262467        en  \n",
      "2    0.405  118.593       243067        en  \n",
      "3    0.240  130.018       193160        en  \n",
      "4    0.305  147.764       224720        en  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "#  Guardamos en un dataframe la data del archivo csv\n",
    "csv_file_path = f\"./Postgres/spotify_songs2.csv\"\n",
    "# Cargar el CSV en un DataFrame de pandas\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Mostrar las primeras filas del archivo\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:04:52.162487Z",
     "start_time": "2024-11-15T23:04:49.904771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.36\n",
      "Conexión exitosa a la base de datos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hacemos la coneccion con PostgreSQL\n",
    "import sqlalchemy as sa\n",
    "print(sa.__version__)\n",
    "import pandas as pd\n",
    "\n",
    "# Crea la conexión a la base de datos\n",
    "# engine = sa.create_engine('postgresql://postgres:1234@localhost:5432/Data')#NO OLVIDAR cambiar el nombre de la base de datos y la contraseña\n",
    "engine = sa.create_engine('postgresql://postgres:postgres@localhost:5432/data')#\n",
    "# Prueba la conexión\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Conexión exitosa a la base de datos.\")\n",
    "except Exception as e:\n",
    "    print(\"Error en la conexión:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:01.544634Z",
     "start_time": "2024-11-15T23:04:59.408371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla 'Canciones' ha sido creada en PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subimos el DataFrame como una tabla en PostgreSQL\n",
    "nombre_tabla = \"Canciones\"\n",
    "df.to_sql(nombre_tabla, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(f\"La tabla '{nombre_tabla}' ha sido creada en PostgreSQL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:10.342606Z",
     "start_time": "2024-11-15T23:05:10.301553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          track_name\n",
      "0                                       I Feel Alive\n",
      "1                                             Poison\n",
      "2  Baby It's Cold Outside (feat. Christina Aguilera)\n",
      "3                                         Dumb Litty\n",
      "4                                            Soldier\n",
      "5                                        Satisfy You\n",
      "6                                   Talk Dirty To Me\n",
      "7                                       Tender Lover\n",
      "8                      Hide Away (feat. Envy Monroe)\n",
      "9                                          Limestone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Consulta de prueba\n",
    "# Consulta de ejemplo: selecciona todas las filas de la tabla \"Canciones\"\n",
    "query = ''' CREATE EXTENSION IF NOT EXISTS pg_trgm;\n",
    "            SELECT track_name FROM public.\"Canciones\" LIMIT 10;'''\n",
    "df_query = pd.read_sql_query(query, engine)\n",
    "print(df_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:21.841778Z",
     "start_time": "2024-11-15T23:05:21.828945Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import text\n",
    "def execute_query(query):\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(query))\n",
    "        connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:29.981671Z",
     "start_time": "2024-11-15T23:05:29.491375Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Concatenar las columnas track_name, track_artist, lyrics\n",
    "query='''ALTER TABLE public.\"Canciones\" ADD COLUMN full_text TEXT;\n",
    "UPDATE public.\"Canciones\"\n",
    "SET full_text= CONCAT(track_name,' ',track_artist,' ',lyrics);'''\n",
    "execute_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:37.902282Z",
     "start_time": "2024-11-15T23:05:34.188111Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Crear un atributo de tipo vector textual\n",
    "query = ''' alter table public.\"Canciones\" add column lyrics_w tsvector; \n",
    "            update public.\"Canciones\" set lyrics_w = to_tsvector(\"full_text\");\n",
    "            '''\n",
    "execute_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:42.663311Z",
     "start_time": "2024-11-15T23:05:42.134626Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Crear un atributo de tipo vector textual\n",
    "# agregar el indice\n",
    "query = '''CREATE INDEX lyrics_w_idx ON public.\"Canciones\" USING gin (lyrics_w);'''\n",
    "execute_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:45.637365Z",
     "start_time": "2024-11-15T23:05:45.619157Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = '''set enable_seqscan = off;'''\n",
    "execute_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:05:52.204531Z",
     "start_time": "2024-11-15T23:05:52.155452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       track_name track_artist  \\\n",
      "0      Good as Hell (feat. Ariana Grande) - Remix        Lizzo   \n",
      "1                                     The Spectre  Alan Walker   \n",
      "2  Body on My (feat. Brando, Pitbull & Nicky Jam)  Loud Luxury   \n",
      "3                                      Body On My  Loud Luxury   \n",
      "4                                 Let Me Love You     DJ Snake   \n",
      "5                               Hello, I Love You    The Doors   \n",
      "6  If The World Was Ending (feat. Julia Michaels)      JP Saxe   \n",
      "7                                 Let Me Love You     DJ Snake   \n",
      "8                                    Good as Hell        Lizzo   \n",
      "9               I Don't Care (with Justin Bieber)   Ed Sheeran   \n",
      "\n",
      "                                              lyrics     score  \n",
      "0  I do my hair toss, check my nails Baby, how yo...  0.097105  \n",
      "1  Hello, hello, can you hear me as I scream your...  0.096063  \n",
      "2  Jaja, everybody knows I'm live (Candela) Every...  0.096011  \n",
      "3  Jaja, everybody knows I'm live (Candela) Every...  0.096011  \n",
      "4  I used to believe We were burnin' on the edge ...  0.095814  \n",
      "5  Hello, I love you Won't you tell me your name?...  0.095390  \n",
      "6  I was distracted and in traffic I didn't feel ...  0.095139  \n",
      "7  I used to believe We were burnin' on the edge ...  0.094522  \n",
      "8  I do my hair toss, check my nails Baby, how yo...  0.094425  \n",
      "9  I'm at a party I don't wanna be at And I don't...  0.094336  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Aplicar las consultas\n",
    "query='''\n",
    "        SELECT track_name, track_artist, lyrics, ts_rank(lyrics_w, query) as score\n",
    "        FROM public.\"Canciones\", to_tsquery('hello | love ') query\n",
    "        WHERE query @@ lyrics_w\n",
    "        order by score DESC\n",
    "        limit 10;\n",
    "      '''\n",
    "df_query = pd.read_sql_query(query, engine)\n",
    "print(df_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser : PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:07:23.028576Z",
     "start_time": "2024-11-15T23:07:23.013222Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def consultasql(text):\n",
    "\n",
    "\n",
    "    with open(r\"./english_stoplist.txt\", encoding=\"utf-8\") as file:\n",
    "        stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "    columnas= r\"select\\s+([^;]*)\\s+from\"  # me da lo que esta entre 'select' y 'from'\n",
    "    palabras = r\"liketo\\s+'(.*?)'\"  # obtiene la o las palabras que estan en comillas\n",
    "    limite= r\"limit\\s+(\\d+)\"  # obtiene el limite\n",
    "    columnaspre = re.search(columnas, text, re.IGNORECASE)\n",
    "    palabraspre = re.search(palabras, text, re.IGNORECASE)\n",
    "    limitepre= re.search(limite, text, re.IGNORECASE)\n",
    "    if columnaspre!=None:\n",
    "        columnasfinal = columnaspre.group(1).strip()\n",
    "    else:\n",
    "        columnasfinal =''\n",
    "    if palabraspre!=None:\n",
    "        palabrasfinal = palabraspre.group(1)\n",
    "    else:\n",
    "        palabrasfinal=''\n",
    "    if limitepre!=None:\n",
    "        limitefinal = limitepre.group(0)\n",
    "    else:\n",
    "        limitefinal=''\n",
    "\n",
    "    palabrasq = palabrasfinal.split()# divido mis palabras\n",
    "    palabrasq = [palabra for palabra in palabrasq if palabra.lower() not in stoplist]# elimino lo que no me da mucha información\n",
    "    palabrasya =\" | \".join(palabrasq)\n",
    "\n",
    "    tsqueri = f\"to_tsquery('{palabrasya}')\"\n",
    "\n",
    "    #aqui esta el procedimiento de la tranformacion del texto a una query para que lo acepte sql y me retorne el resultado\n",
    "    query='''SELECT '''+columnasfinal+''', ts_rank(lyrics_w, query) as score FROM public.\"Canciones\",''' +tsqueri +'''query\n",
    "    WHERE query @@ lyrics_w\n",
    "    order by score DESC '''+limitefinal+''' ;'''\n",
    "\n",
    "    df_query = pd.read_sql_query(query, engine)\n",
    "    return df_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:07:40.900462Z",
     "start_time": "2024-11-15T23:07:40.885106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crumblin' Erb</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>NA Yessuh, let me dig into your brain, folks f...</td>\n",
       "      <td>0.060793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ride Wit Me</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>Where they at? Where they at? Where they at? W...</td>\n",
       "      <td>0.050661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Confessions of a Dangerous Mind</td>\n",
       "      <td>Logic</td>\n",
       "      <td>Yeah I can't get no better, can't get no more ...</td>\n",
       "      <td>0.045595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flick Of The Wrist - Remastered 2011</td>\n",
       "      <td>Queen</td>\n",
       "      <td>\"Dislocate your spine if you don't sign\" he sa...</td>\n",
       "      <td>0.045595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Takin' Over</td>\n",
       "      <td>DJ Khaled</td>\n",
       "      <td>Ohh ohh! (DJ Khaled!) Konvict Music (We The Be...</td>\n",
       "      <td>0.045595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Don't Believe The Hype</td>\n",
       "      <td>Public Enemy</td>\n",
       "      <td>Don't— Don't— Don't— Don't— Don't— Don't— Don'...</td>\n",
       "      <td>0.040528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Killshot</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>You sound like a bitch, bitch Shut the fuck up...</td>\n",
       "      <td>0.040528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ride Wit Me</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>Where they at? Where they at? Where they at? W...</td>\n",
       "      <td>0.040528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hallelujah (feat. Buddy, A$AP Ferg &amp; Wale)</td>\n",
       "      <td>Godfather of Harlem</td>\n",
       "      <td>Yeah, ooh Huh Yeah, hey, uh, ooh Yeah, okay, o...</td>\n",
       "      <td>0.040528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No Shorts, No Losses</td>\n",
       "      <td>Bone Thugs-N-Harmony</td>\n",
       "      <td>Bone come break 'em down Taking no shorts no l...</td>\n",
       "      <td>0.040528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   track_name          track_artist  \\\n",
       "0                               Crumblin' Erb               OutKast   \n",
       "1                                 Ride Wit Me                 Nelly   \n",
       "2             Confessions of a Dangerous Mind                 Logic   \n",
       "3        Flick Of The Wrist - Remastered 2011                 Queen   \n",
       "4                              We Takin' Over             DJ Khaled   \n",
       "5                      Don't Believe The Hype          Public Enemy   \n",
       "6                                    Killshot                Eminem   \n",
       "7                                 Ride Wit Me                 Nelly   \n",
       "8  Hallelujah (feat. Buddy, A$AP Ferg & Wale)   Godfather of Harlem   \n",
       "9                        No Shorts, No Losses  Bone Thugs-N-Harmony   \n",
       "\n",
       "                                              lyrics     score  \n",
       "0  NA Yessuh, let me dig into your brain, folks f...  0.060793  \n",
       "1  Where they at? Where they at? Where they at? W...  0.050661  \n",
       "2  Yeah I can't get no better, can't get no more ...  0.045595  \n",
       "3  \"Dislocate your spine if you don't sign\" he sa...  0.045595  \n",
       "4  Ohh ohh! (DJ Khaled!) Konvict Music (We The Be...  0.045595  \n",
       "5  Don't— Don't— Don't— Don't— Don't— Don't— Don'...  0.040528  \n",
       "6  You sound like a bitch, bitch Shut the fuck up...  0.040528  \n",
       "7  Where they at? Where they at? Where they at? W...  0.040528  \n",
       "8  Yeah, ooh Huh Yeah, hey, uh, ooh Yeah, okay, o...  0.040528  \n",
       "9  Bone come break 'em down Taking no shorts no l...  0.040528  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query ='''select track_name,track_artist,lyrics from Canciones where lyric liketo 'NA Yessuh, let me dig into your brain' limit 10;'''\n",
    "consultasql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Interfaz Gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:27:45.668866Z",
     "start_time": "2024-11-15T22:27:45.665527Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: refactor this function\n",
    "# Wrapper function to use the SPIMI algorithm\n",
    "def wrapper(query, k):\n",
    "    result = index.retrieval(query, k)\n",
    "    return index.displayResults(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:31:02.818411Z",
     "start_time": "2024-11-15T22:31:02.220470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Query tfidf mas: 0.1900499992051599\n",
      "Results retrieved\n",
      "Top 5 songs:\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Testing the wrapper function\n",
    "results_wrapper = wrapper(\"I really can't stay Baby it's cold outside I've got to go away Baby it's cold out there This evening has been Been hoping that you'd drop in So very nice I'll hold your hands, they're just like ice My mother will start to worry Beautiful, what's your hurry? My father will be pacing the floor Listen to that fireplace roar So really I'd better scurry Beautiful, please don't hurry Well maybe just a half a drink more Why don't you put some records on while I pour The neighbors might think Baby, it's bad out there Say, what's in this drink? No cabs to be had out there I wish I knew how Your eyes are like starlight To break the spell I'll take your hat, your hair looks swell I ought to say no, no, no, sir Mind if I move in closer? At least I'm gonna say that I tried What's the sense in hurting my pride? I really can't stay Baby don't hold out Baby it's cold outside I simply must go See that it's cold outside The answer is no I said it's cold out there This welcome has been How lucky that you dropped in So nice and warm Look out the window at that storm My sister will be suspicious Gosh, your lips look delicious My brother will be there at the door Waves upon a tropical shore My maiden aunt's mind is vicious Oh, your lips are delicious Maybe just a cigarette more Never such a blizzard before Hey I've got to go home Baby, you'll freeze out there Say, lend me your coat It's up to your knees out there You've really been grand I'm thrilled when you touch my hand But don't you see How can you do this thing to me? There's bound to be talk tomorrow Think of my life long sorrow At least there will be plenty implied If you caught pneumonia and died I really can't stay Get over that old lie Baby, baby it's cold outside\", 5)\n",
    "print(results_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:16:53.854235Z",
     "start_time": "2024-11-15T23:16:52.241981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# FIXME - ARREGLAR WARNINGS Y ORDENAR LA FUNCION\n",
    "import gradio as gr\n",
    "\n",
    "def interface_1():\n",
    "    \"\"\"\n",
    "    Primera interfaz para el algoritmo SPIMI.\n",
    "    \"\"\"\n",
    "    with gr.Blocks(theme='HaleyCH/HaleyCH_Theme') as demo1:\n",
    "    \n",
    "        demo1.title = \"SPIMI Algorithm\"\n",
    "\n",
    "  \n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Ingrese su consulta aquí...\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            k_input = gr.Slider(\n",
    "                label=\"Top Scores\",\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                value=5,\n",
    "                step=1\n",
    "            )\n",
    "\n",
    "   \n",
    "        with gr.Row():\n",
    "            output_df = gr.DataFrame(\n",
    "                headers=None,\n",
    "                datatype=\"str\",\n",
    "                label=\"Resultados\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\n",
    "                \"Get Results\",\n",
    "                variant=\"primary\"\n",
    "            )\n",
    "\n",
    "   \n",
    "        submit_btn.click(\n",
    "            fn=wrapper,\n",
    "            inputs=[query_input, k_input],\n",
    "            outputs=output_df\n",
    "        )\n",
    "\n",
    "    return demo1\n",
    "\n",
    "def interface_2():\n",
    "    \"\"\"\n",
    "    Segunda interfaz para el índice GIN.\n",
    "    \"\"\"\n",
    "    with gr.Blocks(theme='HaleyCH/HaleyCH_Theme') as demo2:\n",
    "      \n",
    "        demo2.title = \"GIN Index\"\n",
    "\n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Ingrese su consulta aquí...\"\n",
    "            )\n",
    "\n",
    "       \n",
    "        with gr.Row():\n",
    "            output_df = gr.DataFrame(\n",
    "                headers=None,\n",
    "                datatype=\"str\",\n",
    "                label=\"Resultados\"\n",
    "            )\n",
    "\n",
    "    \n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\n",
    "                \"Get Results\",\n",
    "                variant=\"primary\"\n",
    "            )\n",
    "\n",
    "   \n",
    "        submit_btn.click(\n",
    "            fn=lambda query: consultasql(query),\n",
    "            inputs=[query_input],\n",
    "            outputs=output_df\n",
    "        )\n",
    "\n",
    "    return demo2\n",
    "\n",
    "def main_demo():\n",
    "    \"\"\"\n",
    "    Función principal que crea el layout con ambas interfaces\n",
    "    \"\"\"\n",
    "    with gr.Blocks(theme='HaleyCH/HaleyCH_Theme') as main_demo:\n",
    "        gr.Markdown(\"Spotify Songs Search Engine\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                demo1 = interface_1()\n",
    "                \n",
    "            with gr.Column():\n",
    "                demo2 = interface_2()\n",
    "               \n",
    "\n",
    "    main_demo.launch()\n",
    "    return main_demo\n",
    "\n",
    "main_interfaz = main_demo()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T23:23:17.361249Z",
     "start_time": "2024-11-15T23:23:17.229215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7864\n"
     ]
    }
   ],
   "source": [
    "main_interfaz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:55:14.542799Z",
     "start_time": "2024-10-24T09:55:10.846102Z"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=wrapper,\n",
    "    inputs=[gr.Text(label=\"Query\"), gr.Slider(label=\"Top Scores\")],\n",
    "    outputs=[gr.Dataframe(\n",
    "        headers=[\"ID\", \"Url\", \"Score\"],\n",
    "        datatype=[\"str\", \"str\", \"str\"],\n",
    "    )],\n",
    "    title=\"Proyecto DB2\",\n",
    "    examples=[\n",
    "        [\"El pais de China y su cooperacion\", 5],\n",
    "        [\"Salud y bienestar en la sociedad\", 10],\n",
    "    ],\n",
    "    theme='HaleyCH/HaleyCH_Theme'\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "0f6751b6349d4dc18cbbb391b3b45a06",
  "deepnote_persisted_session": {
   "createdAt": "2024-05-23T14:46:04.322Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
