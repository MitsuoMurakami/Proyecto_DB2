{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr2ewyCfSkO1",
    "deepnote_app_block_visible": true,
    "cell_id": "f72575afc9b54cf98a7b62a31329ed15",
    "deepnote_cell_type": "markdown"
   },
   "source": "# SPIMI Algorithm",
   "block_group": "fadba266b21a4c58abdca8ad41c53e19"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "4b61a710a23643a695c0c0082646f4cf",
    "deepnote_cell_type": "markdown"
   },
   "source": "### 1- Estructura del índice invertido en Python:",
   "block_group": "525622409eb5419090a19704f6f60be3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "deepnote_to_be_reexecuted": true,
    "deepnote_app_block_visible": true,
    "cell_id": "c2d154f448dc41d8b348eb9d9c1b48bf",
    "deepnote_cell_type": "code",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:52:39.611088Z",
     "start_time": "2024-10-24T09:52:39.607071Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "El indice esta estructurado de la siguiente forma, contiene el idf para cada termino y su posting list. \n",
    "La posting list contiene el id del documento y la frecuencia del termino en el documento.\n",
    "\n",
    "index.txt :\n",
    "\n",
    "w1 : idf_w1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\n",
    "w2 : idf_w2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\n",
    "w3 : idf_w3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\n",
    "\n",
    "position_terms.pkl:\n",
    "\n",
    "length ={\n",
    "doc1: norm_doc1,\n",
    "doc2: norm_doc2,\n",
    "doc3: norm_doc3,\n",
    "...\n",
    "}\n",
    "\"\"\""
   ],
   "block_group": "4a931760a1684d6d98c15bfd62be9027",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nindex = {\\nw1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\\nw2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\\nw3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\\n}\\n\\nidf = {\\nw1 : idf_w1,\\nw2 : idf_w2,\\nw3 : idf_w3,\\n}\\n\\nlength ={\\ndoc1: norm_doc1,\\ndoc2: norm_doc2,\\ndoc3: norm_doc3,\\n...\\n}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "5ccacf43c13742dbaf088ceb45b5c621",
    "deepnote_cell_type": "markdown"
   },
   "source": "# SPIMI ",
   "block_group": "87ecd9a159724ec4a420f04e467a3c32"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "d90d743f",
    "execution_start": 1728075206132,
    "execution_millis": 0,
    "execution_context_id": "1e7e5f1c-d694-4140-b3f8-562293cc6b23",
    "deepnote_app_block_visible": true,
    "cell_id": "dca6b48d30794b3e86f5da878ce5451b",
    "deepnote_cell_type": "code",
    "ExecuteTime": {
     "end_time": "2024-11-15T22:25:00.257203Z",
     "start_time": "2024-11-15T22:25:00.235674Z"
    }
   },
   "source": [
    "import pickle\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import os\n",
    "import sys\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class InvertIndexSPIMI:\n",
    "    def __init__(self):\n",
    "      \n",
    "        self.index_main = {} # dictionary to store the position of each term in the index file\n",
    "        self.length_main = {} # dictionary to store the norm for each document\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stoplist = []\n",
    "        self.preprocessed_words_file = \"preprocessed_words.txt\"\n",
    "        self.spimi_file = \"spimi_blocks.txt\"\n",
    "        self.id_block = 0\n",
    "        self.free_memory_available = 500000  # 0.5 MB\n",
    "        self.current_line_preprocessed_words_file = 0\n",
    "        self.end_preprocessed_words_file = 0\n",
    "        self.min_heap_size = 500\n",
    "        self.index = \"index.txt\"\n",
    "        self.cant_documents = 0\n",
    "        self.norma_docs_file = \"norma_docs.pkl\" \n",
    "        self.position_terms_file = \"position_terms.pkl\"\n",
    "        self.original_file = \"\"\n",
    "        #fd\n",
    " # TODO Process the text to create many blocks using the SPIMI algorithm\n",
    "    def load_stop_list(self, filename=\"english_stoplist.txt\"):\n",
    "        with open(filename, encoding=\"utf-8\") as file:\n",
    "            self.stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "    def preprocesamiento_aux(self, texto):\n",
    "        words = []\n",
    "        # tokenizar\n",
    "        texto_tok = texto.lower()\n",
    "        texto_tok = re.sub(r'[^a-zA-Z_À-ÿ]', ' ', texto)\n",
    "        words = nltk.word_tokenize(texto_tok, language='english')\n",
    "        # filtrar stopwords\n",
    "        words = [word for word in words if word not in self.stoplist]\n",
    "        # reducir palabras\n",
    "        words = [self.stemmer.stem(word) for word in words]\n",
    "        return words\n",
    "    def get_list_tf(self, tokens, id):\n",
    "        # compute the term frequency for each word in the document with id\n",
    "        tf = {}\n",
    "        for word in tokens:\n",
    "            if word not in tf:\n",
    "                tf[word] = 1\n",
    "            else:\n",
    "                tf[word] += 1\n",
    "        return [(id, word, tf[word]) for word in tf]\n",
    "    # process the text to create the preprocessed words file\n",
    "    def preprocesamiento(self, pathOfFile):\n",
    "        print(\"Preprocessing songs\")      \n",
    "        self.original_file = pathOfFile\n",
    "        # List of tokens for each song\n",
    "        preprocessed_song = []           \n",
    "        with open(pathOfFile, newline='', encoding='utf-8') as file, open(self.preprocessed_words_file, 'w') as preprocessed_words:\n",
    "            songs_file = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "            # counter used as id for each song\n",
    "            counter_rows = 0\n",
    "            for row in songs_file:\n",
    "                # skip the header\n",
    "                if counter_rows == 0:\n",
    "                    counter_rows += 1\n",
    "                    continue\n",
    "                # track_name , artist_name, lyrics\n",
    "                song_details = row[1] + \" \" + row[2] + \" \" + row[3]\n",
    "                # get list of tokens for each song\n",
    "                preprocessed_song = self.preprocesamiento_aux(song_details)\n",
    "                # get the tf for each word in the song\n",
    "                list_tf_song = self.get_list_tf(preprocessed_song, counter_rows)\n",
    "                counter_rows += 1\n",
    "                # write the relative_id_song, word, tf in each line of file\n",
    "                for tf in list_tf_song:\n",
    "                    preprocessed_words.write(str(tf) + \"\\n\")\n",
    "            self.cant_documents = counter_rows\n",
    "          \n",
    "    def strToList(self, string):\n",
    "        string = string[:-1] \n",
    "        string = string[1:-1]\n",
    "        tup = string.split(\", \")\n",
    "        return [int(tup[0]), tup[1], int(tup[2])]\n",
    "    \n",
    "    # receive the token list which is a list of tuples (id, word, tf)\n",
    "    def spimi_invert(self):\n",
    "        blocks_directory = r\".\\blocks\"\n",
    "        if not os.path.exists(blocks_directory):\n",
    "            os.makedirs(blocks_directory)\n",
    "        output_file = r\".\\blocks\\block_\" + str(self.id_block) + \".txt\"\n",
    "        dictionary = {}\n",
    "        with open(self.preprocessed_words_file, 'r') as preprocessed_words, open(output_file, 'w') as block:\n",
    "            print(\"In block:\", self.id_block)\n",
    "            preprocessed_words.seek(self.current_line_preprocessed_words_file)\n",
    "            while sys.getsizeof(dictionary) < self.free_memory_available:\n",
    "                # read the next token\n",
    "                token = preprocessed_words.readline()\n",
    "                if not token:\n",
    "                    self.end_preprocessed_words_file = 1\n",
    "                    break\n",
    "                # convert the string to a list of relative_id_song, word and its tf\n",
    "                token = self.strToList(token)\n",
    "                if token[1] not in dictionary:\n",
    "                    dictionary[token[1]] = [(token[0], token[2])]\n",
    "                else:\n",
    "                    dictionary[token[1]].append((token[0], token[2]))\n",
    "            self.current_line_preprocessed_words_file = preprocessed_words.tell()\n",
    "            sorted_terms = sorted(dictionary.keys())\n",
    "            for term in sorted_terms:\n",
    "                block.write(term + \":\" + str(dictionary[term]) + \"\\n\")\n",
    "        self.id_block += 1\n",
    "        \n",
    "        \n",
    "    def create_spimi_blocks(self): \n",
    "        print(\"Creating SPIMI blocks\")\n",
    "        # create a directory to store the blocks\n",
    "        blocks_directory = r\".\\blocks\"\n",
    "        if not os.path.exists(blocks_directory):\n",
    "            os.makedirs(blocks_directory)\n",
    "        # read the preprocessed words file as it has the tf for each token in each song, then created size \n",
    "        # limit blocks with spimi algorithm \n",
    "        while(not self.end_preprocessed_words_file):  \n",
    "            self.spimi_invert()\n",
    "       \n",
    "   \n",
    "    \"\"\"\n",
    "    Para el merge se abriran todos los bloques simultaneamente, se leera la primera palabras de cada bloque,\n",
    "    luego usando una pritority queue se obtendra el menor termino lexicografico y se combinaran todas las \n",
    "    listas existentes para dicho termino. Se escribira en un nuevo archivo y se repetira el proceso hasta \n",
    "    que no haya mas.\n",
    "    Dado que se el minHeap se inicializa con 1 termino de cada bloque y cada vez que se hace pop, se pushea \n",
    "    un term del mismo bloque, se garantiza que el heap tiene un term de cada bloque a menos que se haya llegado\n",
    "    al final de un bloque. Ademas, se itera hasta que el heap este vacio, por lo que se garantiza el \n",
    "    recorrido total por todos los bloques.\n",
    "    \"\"\"\n",
    "   \n",
    "    def merge_spimi_blocks(self):\n",
    "        print(\"Merging SPIMI blocks\")\n",
    "        # get all the block names\n",
    "        blocks_name = os.listdir(r\".\\blocks\")\n",
    "        # open all the blocks\n",
    "        opened_blocks = []\n",
    "        for block_name in blocks_name:\n",
    "            opened_blocks.append(open(\".\\\\blocks\\\\\" + block_name, 'r'))\n",
    "        # min heap for terms\n",
    "        terms_from_all_blocks = []\n",
    "        # dictionary to store the norm for each document\n",
    "        temp_norm_dict = {}\n",
    "        # dictionary to store the position of each term in the index file\n",
    "        position_terms = {}\n",
    "        # read the first term from each block\n",
    "        counter_opened_block = 0\n",
    "        for opened_block in opened_blocks:\n",
    "            term = opened_block.readline()\n",
    "            if term:\n",
    "                term = term.split(\":\")\n",
    "                # push (term, [...], block_number) to the heap\n",
    "                heapq.heappush(terms_from_all_blocks, (term[0], term[1], counter_opened_block))\n",
    "            counter_opened_block += 1\n",
    "        # merge the blocks in index.txt\n",
    "        with open(self.index, 'w') as index:\n",
    "            prev_top_term = None\n",
    "            temp_prev_top_term = []\n",
    "            \n",
    "            # iterate until the heap is empty\n",
    "            while len(terms_from_all_blocks) > 0:\n",
    "                # obtenemos el menor termino lexicograficamente \n",
    "                current_top_term = heapq.heappop(terms_from_all_blocks)\n",
    "                # if the previous term is the same as the current term, merge their posting lists\n",
    "                if prev_top_term and prev_top_term[0] == current_top_term[0]:\n",
    "                    # convert the string to a list of tuples if necessary\n",
    "                    if isinstance(prev_top_term[1], str):                \n",
    "                        temp_prev_top_term = eval(prev_top_term[1])\n",
    "                    # posting list of current term is always a str\n",
    "                    temp_current_top_term = eval(current_top_term[1])\n",
    "                    # merge posting lists\n",
    "                    temp_prev_top_term.extend(temp_current_top_term)\n",
    "                    # update the posting list and block number \n",
    "                    prev_top_term =(prev_top_term[0], temp_prev_top_term, current_top_term[2])\n",
    "                    \n",
    "                else:\n",
    "                    prev_top_term = current_top_term\n",
    "                    # transform to List if posting list is a str\n",
    "                    if isinstance(current_top_term[1], str):\n",
    "                        temp_posting_list = eval(current_top_term[1])\n",
    "                        current_top_term = (current_top_term[0], temp_posting_list, current_top_term[2])\n",
    "                    # compute the idf\n",
    "                    idf_t = np.log10(self.cant_documents / len(current_top_term[1]))\n",
    "                    # compute the tf_idf squared weights of each term for each document, doc is a tuple (doc_id, tf)\n",
    "                    for doc in current_top_term[1]:\n",
    "                        if doc[0] not in temp_norm_dict:\n",
    "                            temp_norm_dict[doc[0]] = ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                        else:\n",
    "                            temp_norm_dict[doc[0]] += ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                    \n",
    "                    # store the position of the term in the index file\n",
    "                    position_terms[current_top_term[0][1:-1]] = index.tell()\n",
    "                    # write the term to the index -> term:idf:posting_list \n",
    "                    index.write(current_top_term[0] + \":\" + str(idf_t) + \":\" + str(current_top_term[1]) + \"\\n\")\n",
    "                    \n",
    "                # read the next term from the block of the current term\n",
    "                next_term_same_block = opened_blocks[current_top_term[2]].readline()\n",
    "                if next_term_same_block:\n",
    "                       next_term_same_block = next_term_same_block.split(\":\")\n",
    "                       # push the next term to the heap\n",
    "                       heapq.heappush(terms_from_all_blocks, (next_term_same_block[0], next_term_same_block[1], current_top_term[2]))\n",
    "        \n",
    "        # compute the norm of each document\n",
    "        for doc in temp_norm_dict:\n",
    "            temp_norm_dict[doc] = np.sqrt(temp_norm_dict[doc])\n",
    "        print(\"Blocks merged\")\n",
    "        # write norms to disk with pickle to recover them later in dictionary format\n",
    "        with open(self.norma_docs_file, 'wb') as file:\n",
    "            pickle.dump(temp_norm_dict, file)\n",
    "        # write the position of each term in the index file to disk with pickle to recover them later\n",
    "        with open(self.position_terms_file, 'wb') as file:\n",
    "            pickle.dump(position_terms, file)\n",
    "        # close blocks\n",
    "        for opened_block in opened_blocks:\n",
    "            opened_block.close()\n",
    "        print(\"Index created\")  \n",
    "    # build the index\n",
    "    def build (self, pathOfFile):\n",
    "        # FIXME: Only build the process if the index file does not exist for development purposes\n",
    "        if os.path.exists(self.index):\n",
    "            return\n",
    "        self.load_stop_list()\n",
    "        self.preprocesamiento(pathOfFile)\n",
    "        self.create_spimi_blocks()\n",
    "        self.merge_spimi_blocks()\n",
    "    \n",
    "    def load_index(self):\n",
    "        # load index (position of terms in index file) and the norm of documents from disk       \n",
    "        try:\n",
    "            with open(self.position_terms_file, 'rb') as file:\n",
    "                self.index_main = pickle.load(file)\n",
    "                print(\"Index main ready\")\n",
    "            with open(self.norma_docs_file, 'rb') as file:\n",
    "                self.length_main = pickle.load(file)\n",
    "                print(\"Length main ready\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Index not found\")\n",
    "\n",
    "        \n",
    "    def retrieval(self, query, k):\n",
    "        # load pos terms and norm docs\n",
    "        self.load_index()\n",
    "        # diccionario para el score\n",
    "        score = {}\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        post_query = self.preprocesamiento_aux(query)\n",
    "        # calcular el tf-idf del query\n",
    "        query_words = set(post_query)\n",
    "        query_tfidf = []\n",
    "        with open(self.index, 'r') as idx_file:\n",
    "            for wordq in query_words:\n",
    "                idfq = 0\n",
    "                if wordq in self.index_main:\n",
    "                    # posicion del termino en el index \n",
    "                    pos_wordq = self.index_main[wordq]\n",
    "                    # read the term from the file\n",
    "                    idx_file.seek(pos_wordq)\n",
    "                    term_info = idx_file.readline().split(\":\")\n",
    "                    # idf del termino\n",
    "                    idfq = float(term_info[1])\n",
    "              \n",
    "                  \n",
    "                # tf.idf del termino en el query \n",
    "                query_tfidf.append((1 + np.log10(post_query.count(wordq))) * idfq)\n",
    "        # compute the norm of the query \n",
    "       \n",
    "        query_norm = np.sqrt(sum([q ** 2 for q in query_tfidf]))\n",
    "        # normalizar la query\n",
    "       \n",
    "        query_tfidf = [q / query_norm for q in query_tfidf]\n",
    "        query_words = list(query_words)\n",
    "\n",
    "\n",
    "        temp__ = {}\n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "        with open(self.index, 'r') as idx_file:\n",
    "            for word in query_words:\n",
    "                if word in self.index_main:\n",
    "                    pos_word = self.index_main[word]\n",
    "                    idx_file.seek(pos_word)\n",
    "                    term_info = idx_file.readline().split(\":\")\n",
    "                    idf = float(term_info[1])\n",
    "                    posting_list_ = eval(term_info[2])\n",
    "               \n",
    "                    for doc, tf in posting_list_:\n",
    "                        if doc not in score:\n",
    "                            score[doc] = 0\n",
    "                            temp__[doc] = 0\n",
    "                        # tf.idf del termino en el documento por el tf.idf del termino en el query\n",
    "                        tf_idf_ = (1 + np.log10(tf)) * idf\n",
    "                        score[doc] += query_tfidf[query_words.index(word)] * tf_idf_\n",
    "                        temp__[doc] +=(tf*idf)\n",
    "            \n",
    "           \n",
    "            for d in score:\n",
    "                # normalizar el score\n",
    "                score[d] = score[d] / self.length_main[d]\n",
    "                temp__[d] = temp__[d] / self.length_main[d]\n",
    "        \n",
    "        # transform the dictionary to a list of tuples and sort it by the score \n",
    "        result = sorted(score.items(), key=lambda tup: tup[1], reverse=True) \n",
    "        # return only the keys\n",
    "        # result = [key for key, value in result]\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        print(\"Results retrieved\")\n",
    "        print(\"Top\", k, \"songs:\")\n",
    "        return result[:k]\n",
    "    # FIXME 2: display the results in a dataframe\n",
    "    def displayResults(self, result):\n",
    "       \n",
    "        track_ids = []\n",
    "        track_name = []\n",
    "        artists = []\n",
    "        lyrics = []\n",
    "        scores = []\n",
    "        \n",
    "        # open the file with the songs\n",
    "        songs = pd.read_csv(self.original_file, delimiter=',', quotechar='\"')\n",
    "        for item in result:\n",
    "            # info_new = \"Rank:\" + str(i)+ \"Url :\"+ dataton[\"url\"][resuldato[0]]+ \"score: \"+ str(resuldato[1])\n",
    "            #result_news.append(info_new)\n",
    "            \n",
    "            song = songs.iloc[item[0]- 1] \n",
    "            track_ids.append(song['track_id'])\n",
    "            track_name.append(song['track_name'])\n",
    "            artists.append(song['track_artist'])\n",
    "            lyrics.append(song['lyrics'])\n",
    "            scores.append(str(item[1]))\n",
    "       \n",
    "        df_news = pd.DataFrame({\"ID\": track_ids, \"Track Name\": track_name,\"Artist\": artists ,\"Lyrics\": lyrics, \"Score\": scores})\n",
    "        return df_news\n",
    "\n",
    " # \n"
   ],
   "block_group": "c2e0cdc0e8ff4c99935a31d2e0cfbfb0",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:26:08.365662Z",
     "start_time": "2024-11-15T22:25:06.205568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test preprocess songs\n",
    "index = InvertIndexSPIMI()\n",
    "index.build(r\".\\Postgres\\spotify_songs2.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing songs\n",
      "Creating SPIMI blocks\n",
      "In block: 0\n",
      "In block: 1\n",
      "In block: 2\n",
      "Merging SPIMI blocks\n",
      "Blocks merged\n",
      "Index created\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:27:15.096605Z",
     "start_time": "2024-11-15T22:27:15.070170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test retrieval\n",
    "# TODO: try gradio, display the results in a dataframe\n",
    "# TODO: integrate with the gui of postgres\n",
    "# TODO: delete comments, clean\n",
    "index.retrieval(\"love\", 5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Query tfidf mas: 1.0\n",
      "Results retrieved\n",
      "Top 5 songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(827, np.float64(1.0)),\n",
       " (3118, np.float64(0.9735013010153005)),\n",
       " (1184, np.float64(0.9183348199849688)),\n",
       " (3370, np.float64(0.9050890673717692)),\n",
       " (785, np.float64(0.8583978883774164))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:27:24.558527Z",
     "start_time": "2024-11-15T22:27:24.170332Z"
    }
   },
   "cell_type": "code",
   "source": "index.displayResults(index.retrieval(\"Outkast\", 10))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Query tfidf mas: 1.0\n",
      "Results retrieved\n",
      "Top 10 songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       ID                               Track Name  \\\n",
       "0  5ryUbYQzdRRw0iJ2P5Or55                               Funky Ride   \n",
       "1  7xkt4VVkUfC7MLtb2axxiC       Hey Up There (feat. Ty Dolla $ign)   \n",
       "2  6gEejYQZRMeMODb5rxwdK8                        Myintrotoletuknow   \n",
       "3  7nfGrjZf34FxqS0FWXM5B7                                  Wailin'   \n",
       "4  5qKqRkWnRTXNVWbEOOmGxk                          Wheelz of Steel   \n",
       "5  5voPrNQYQ4kQoRZ9A2NghB  Bowtie (feat. Sleepy Brown & Jazze Pha)   \n",
       "6  5yTQ7Vy6EwKwB64Vho90LI                            West Savannah   \n",
       "7  62NQ8JoW3yfJ2WSmlnaogw                         Born In The Trap   \n",
       "8  6bUNEbXT7HovLW6BgPCBsb                                    Roses   \n",
       "9  6gQvUs5abDFspk9LAlCfKa                          Thought Process   \n",
       "\n",
       "       Artist                                             Lyrics  \\\n",
       "0     OutKast  Let me take you on a funky ride All around the...   \n",
       "1       Buddy  Hey up there, I'm on my way up Tell the radio ...   \n",
       "2     OutKast  Time and time again see I be thinking about th...   \n",
       "3     OutKast  In the zone like Keyser Söze, always the Usual...   \n",
       "4     OutKast  Uh, as I sit in my b-boy stance With flip-flop...   \n",
       "5     OutKast  Girl you cut up (Lucius Left Foot) Girl you kn...   \n",
       "6     OutKast  Yeah, I'm back off in this bitch one more time...   \n",
       "7    The Game  I was born in the crosshairs without a pot to ...   \n",
       "8     OutKast  Caroline (Caroline), see, Caroline All the guy...   \n",
       "9  Goodie Mob  Let me get a chop at this lumber Niggas from d...   \n",
       "\n",
       "                 Score  \n",
       "0  0.18812117562945896  \n",
       "1  0.12389941978016338  \n",
       "2  0.12210220578149839  \n",
       "3  0.10369982450419457  \n",
       "4    0.097631487356621  \n",
       "5   0.0926095476092194  \n",
       "6  0.08975331579503788  \n",
       "7  0.08587607674530255  \n",
       "8  0.07874564466918879  \n",
       "9  0.07701821742684782  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ryUbYQzdRRw0iJ2P5Or55</td>\n",
       "      <td>Funky Ride</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Let me take you on a funky ride All around the...</td>\n",
       "      <td>0.18812117562945896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7xkt4VVkUfC7MLtb2axxiC</td>\n",
       "      <td>Hey Up There (feat. Ty Dolla $ign)</td>\n",
       "      <td>Buddy</td>\n",
       "      <td>Hey up there, I'm on my way up Tell the radio ...</td>\n",
       "      <td>0.12389941978016338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6gEejYQZRMeMODb5rxwdK8</td>\n",
       "      <td>Myintrotoletuknow</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Time and time again see I be thinking about th...</td>\n",
       "      <td>0.12210220578149839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7nfGrjZf34FxqS0FWXM5B7</td>\n",
       "      <td>Wailin'</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>In the zone like Keyser Söze, always the Usual...</td>\n",
       "      <td>0.10369982450419457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5qKqRkWnRTXNVWbEOOmGxk</td>\n",
       "      <td>Wheelz of Steel</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Uh, as I sit in my b-boy stance With flip-flop...</td>\n",
       "      <td>0.097631487356621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5voPrNQYQ4kQoRZ9A2NghB</td>\n",
       "      <td>Bowtie (feat. Sleepy Brown &amp; Jazze Pha)</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Girl you cut up (Lucius Left Foot) Girl you kn...</td>\n",
       "      <td>0.0926095476092194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5yTQ7Vy6EwKwB64Vho90LI</td>\n",
       "      <td>West Savannah</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Yeah, I'm back off in this bitch one more time...</td>\n",
       "      <td>0.08975331579503788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62NQ8JoW3yfJ2WSmlnaogw</td>\n",
       "      <td>Born In The Trap</td>\n",
       "      <td>The Game</td>\n",
       "      <td>I was born in the crosshairs without a pot to ...</td>\n",
       "      <td>0.08587607674530255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6bUNEbXT7HovLW6BgPCBsb</td>\n",
       "      <td>Roses</td>\n",
       "      <td>OutKast</td>\n",
       "      <td>Caroline (Caroline), see, Caroline All the guy...</td>\n",
       "      <td>0.07874564466918879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6gQvUs5abDFspk9LAlCfKa</td>\n",
       "      <td>Thought Process</td>\n",
       "      <td>Goodie Mob</td>\n",
       "      <td>Let me get a chop at this lumber Niggas from d...</td>\n",
       "      <td>0.07701821742684782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2- Interfaz Gráfica"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:27:45.668866Z",
     "start_time": "2024-11-15T22:27:45.665527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: refactor this function\n",
    "def wrapper(query, k):\n",
    "    result = index.retrieval(query, k)\n",
    "    return index.displayResults(result)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:31:02.818411Z",
     "start_time": "2024-11-15T22:31:02.220470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the wrapper function\n",
    "results_wrapper = wrapper(\"I really can't stay Baby it's cold outside I've got to go away Baby it's cold out there This evening has been Been hoping that you'd drop in So very nice I'll hold your hands, they're just like ice My mother will start to worry Beautiful, what's your hurry? My father will be pacing the floor Listen to that fireplace roar So really I'd better scurry Beautiful, please don't hurry Well maybe just a half a drink more Why don't you put some records on while I pour The neighbors might think Baby, it's bad out there Say, what's in this drink? No cabs to be had out there I wish I knew how Your eyes are like starlight To break the spell I'll take your hat, your hair looks swell I ought to say no, no, no, sir Mind if I move in closer? At least I'm gonna say that I tried What's the sense in hurting my pride? I really can't stay Baby don't hold out Baby it's cold outside I simply must go See that it's cold outside The answer is no I said it's cold out there This welcome has been How lucky that you dropped in So nice and warm Look out the window at that storm My sister will be suspicious Gosh, your lips look delicious My brother will be there at the door Waves upon a tropical shore My maiden aunt's mind is vicious Oh, your lips are delicious Maybe just a cigarette more Never such a blizzard before Hey I've got to go home Baby, you'll freeze out there Say, lend me your coat It's up to your knees out there You've really been grand I'm thrilled when you touch my hand But don't you see How can you do this thing to me? There's bound to be talk tomorrow Think of my life long sorrow At least there will be plenty implied If you caught pneumonia and died I really can't stay Get over that old lie Baby, baby it's cold outside\", 5)\n",
    "print(results_wrapper)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Query tfidf mas: 0.1900499992051599\n",
      "Results retrieved\n",
      "Top 5 songs:\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:55:14.542799Z",
     "start_time": "2024-10-24T09:55:10.846102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=wrapper,\n",
    "    inputs=[gr.Text(label=\"Query\"), gr.Slider(label=\"Top Scores\")],\n",
    "    outputs=[gr.Dataframe(\n",
    "        headers=[\"ID\", \"Url\", \"Score\"],\n",
    "        datatype=[\"str\", \"str\", \"str\"],\n",
    "    )],\n",
    "    title=\"Proyecto DB2\",\n",
    "    examples=[\n",
    "        [\"El pais de China y su cooperacion\", 5],\n",
    "        [\"Salud y bienestar en la sociedad\", 10],\n",
    "    ],\n",
    "    theme='HaleyCH/HaleyCH_Theme'\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:55:35.366599Z",
     "start_time": "2024-10-24T09:55:35.193571Z"
    }
   },
   "cell_type": "code",
   "source": "# demo.close()\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:55:17.285325Z",
     "start_time": "2024-10-24T09:55:16.795352Z"
    }
   },
   "cell_type": "code",
   "source": "demo.launch()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_persisted_session": {
   "createdAt": "2024-05-23T14:46:04.322Z"
  },
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_notebook_id": "0f6751b6349d4dc18cbbb391b3b45a06",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
