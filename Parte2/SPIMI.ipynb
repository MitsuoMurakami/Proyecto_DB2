{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr2ewyCfSkO1",
    "deepnote_app_block_visible": true,
    "cell_id": "f72575afc9b54cf98a7b62a31329ed15",
    "deepnote_cell_type": "markdown"
   },
   "source": "# SPIMI Algorithm",
   "block_group": "fadba266b21a4c58abdca8ad41c53e19"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "4b61a710a23643a695c0c0082646f4cf",
    "deepnote_cell_type": "markdown"
   },
   "source": "### 1- Estructura del índice invertido en Python:",
   "block_group": "525622409eb5419090a19704f6f60be3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "deepnote_to_be_reexecuted": true,
    "deepnote_app_block_visible": true,
    "cell_id": "c2d154f448dc41d8b348eb9d9c1b48bf",
    "deepnote_cell_type": "code",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:52:39.611088Z",
     "start_time": "2024-10-24T09:52:39.607071Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "El indice esta estructurado de la siguiente forma, contiene el idf para cada termino y su posting list. \n",
    "La posting list contiene el id del documento y la frecuencia del termino en el documento.\n",
    "\n",
    "index.txt :\n",
    "\n",
    "w1 : idf_w1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\n",
    "w2 : idf_w2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\n",
    "w3 : idf_w3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\n",
    "\n",
    "position_terms.pkl:\n",
    "\n",
    "length ={\n",
    "doc1: norm_doc1,\n",
    "doc2: norm_doc2,\n",
    "doc3: norm_doc3,\n",
    "...\n",
    "}\n",
    "\"\"\""
   ],
   "block_group": "4a931760a1684d6d98c15bfd62be9027",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nindex = {\\nw1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\\nw2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\\nw3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\\n}\\n\\nidf = {\\nw1 : idf_w1,\\nw2 : idf_w2,\\nw3 : idf_w3,\\n}\\n\\nlength ={\\ndoc1: norm_doc1,\\ndoc2: norm_doc2,\\ndoc3: norm_doc3,\\n...\\n}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deepnote_app_block_visible": true,
    "cell_id": "5ccacf43c13742dbaf088ceb45b5c621",
    "deepnote_cell_type": "markdown"
   },
   "source": "# SPIMI ",
   "block_group": "87ecd9a159724ec4a420f04e467a3c32"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "d90d743f",
    "execution_start": 1728075206132,
    "execution_millis": 0,
    "execution_context_id": "1e7e5f1c-d694-4140-b3f8-562293cc6b23",
    "deepnote_app_block_visible": true,
    "cell_id": "dca6b48d30794b3e86f5da878ce5451b",
    "deepnote_cell_type": "code",
    "ExecuteTime": {
     "end_time": "2024-11-18T20:18:24.561074Z",
     "start_time": "2024-11-18T20:18:24.538571Z"
    }
   },
   "source": [
    "import pickle\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class InvertIndexSPIMI:\n",
    "    def __init__(self):\n",
    "      \n",
    "        self.index_main = {} # dictionary to store the position of each term in the index file\n",
    "        self.length_main = {} # dictionary to store the norm for each document \n",
    "        self.stoplist = {}\n",
    "        self.preprocessed_words_file = \"preprocessed_words.txt\"\n",
    "        self.spimi_file = \"spimi_blocks.txt\"\n",
    "        self.id_block = 0\n",
    "        self.free_memory_available = 500000  # 0.5 MB\n",
    "        # self.free_memory_available = 50000\n",
    "        self.current_line_preprocessed_words_file = 0\n",
    "        self.end_preprocessed_words_file = 0\n",
    "        self.min_heap_size = 500\n",
    "        self.index = \"index.txt\"\n",
    "        self.cant_documents = 0\n",
    "        self.norma_docs_file = \"norma_docs.pkl\" \n",
    "        self.position_terms_file = \"position_terms.pkl\"\n",
    "        self.original_file = \"\"\n",
    "        self.abbreviations = None\n",
    "     \n",
    "        #fd\n",
    " \n",
    "    def load_stop_list(self):\n",
    "        # map the abbreviation of the languages supported by nltk.stem.snowball and punkt tokenizer to the stemmer\n",
    "        self.abbreviations = { \n",
    "        \n",
    "            \"da\": \"danish\",\n",
    "            \"nl\": \"dutch\",\n",
    "            \"en\": \"english\",\n",
    "            \"fi\": \"finnish\",\n",
    "            \"fr\": \"french\",\n",
    "            \"de\": \"german\",\n",
    "         \n",
    "            \"it\": \"italian\",\n",
    "            \"no\": \"norwegian\",\n",
    "            \"pt\": \"portuguese\",\n",
    "          \n",
    "            \"ru\": \"russian\",\n",
    "            \"es\": \"spanish\",\n",
    "            \"sv\": \"swedish\"\n",
    "        } \n",
    "\n",
    "        stoplists_files = os.listdir(r\".\\stoplists\") \n",
    "        # load the stoplist for each language supported by nltk\n",
    "        for file in stoplists_files:\n",
    "            language = file.split(\".\")[0]\n",
    "            with open(r\".\\stoplists\\\\\" + file, encoding=\"utf-8\") as file:\n",
    "                self.stoplist[language] = [line.rstrip().lower() for line in file]\n",
    "        \n",
    "        \n",
    "    # TODO: add support to all of the languages in NLTK\n",
    "    def preprocesamiento_aux(self, texto, language_abbreviation=\"en\"):\n",
    "        \n",
    "        # get the stemmer for the language, by default english\n",
    "        language = \"english\"\n",
    "        stoplist_lang = \"en\"\n",
    "        if self.abbreviations.get(language_abbreviation) is not None:\n",
    "            language = self.abbreviations[language_abbreviation]\n",
    "            stoplist_lang = language_abbreviation\n",
    "        stemmer = SnowballStemmer(language) \n",
    "        words = []\n",
    "        # tokenizar\n",
    "        texto_tok = texto.lower()\n",
    "        texto_tok = re.sub(r\"[^a-zA-ZÀ-ÿ]\", ' ', texto_tok)\n",
    "        words = nltk.word_tokenize(texto_tok, language=language)\n",
    "        # filtrar stopwords\n",
    "        words = [word for word in words if word not in self.stoplist[stoplist_lang]]\n",
    "        # reducir palabras\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        return words\n",
    "    def get_list_tf(self, tokens, id):\n",
    "        # compute the term frequency for each word in the document with id\n",
    "        tf = {}\n",
    "        for word in tokens:\n",
    "            if word not in tf:\n",
    "                tf[word] = 1\n",
    "            else:\n",
    "                tf[word] += 1\n",
    "        return [(id, word, tf[word]) for word in tf]\n",
    "    # process the text to create the preprocessed words file\n",
    "    def preprocesamiento(self, pathOfFile):\n",
    "        print(\"Preprocessing songs\")      \n",
    "        self.original_file = pathOfFile\n",
    "        # List of tokens for each song\n",
    "        preprocessed_song = []           \n",
    "        with open(pathOfFile, newline='', encoding='utf-8') as file, open(self.preprocessed_words_file, 'w') as preprocessed_words:\n",
    "            songs_file = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "            # counter used as id for each song\n",
    "            counter_rows = 0\n",
    "            for row in songs_file:\n",
    "                # skip the header\n",
    "                if counter_rows == 0:\n",
    "                    counter_rows += 1\n",
    "                    continue\n",
    "                # track_name , artist_name, lyrics\n",
    "                song_details = row[1] + \" \" + row[2] + \" \" + row[3]\n",
    "                # get list of tokens for each song, send the song details and language\n",
    "                preprocessed_song = self.preprocesamiento_aux(song_details, row[24])\n",
    "                # get the tf for each word in the song\n",
    "                list_tf_song = self.get_list_tf(preprocessed_song, counter_rows)\n",
    "                counter_rows += 1\n",
    "                # write the relative_id_song, word, tf in each line of file\n",
    "                for tf in list_tf_song:\n",
    "                    preprocessed_words.write(str(tf) + \"\\n\")\n",
    "            self.cant_documents = counter_rows\n",
    "          \n",
    "    def strToList(self, string):\n",
    "        string = string[:-1] # remove the \\n \n",
    "        string = string[1:-1] # remove the brackets\n",
    "        tup = string.split(\", \") # split the string by the comma\n",
    "        return [int(tup[0]), tup[1][1:-1], int(tup[2])]\n",
    "    \n",
    "    # receive the token list which is a list of tuples (id, word, tf)\n",
    "    def spimi_invert(self):\n",
    "        blocks_directory = r\".\\blocks\"\n",
    "        if not os.path.exists(blocks_directory):\n",
    "            os.makedirs(blocks_directory)\n",
    "        output_file = r\".\\blocks\\block_\" + str(self.id_block) + \".txt\"\n",
    "        dictionary = {}\n",
    "        with open(self.preprocessed_words_file, 'r') as preprocessed_words, open(output_file, 'w') as block:\n",
    "            print(\"In block:\", self.id_block)\n",
    "            preprocessed_words.seek(self.current_line_preprocessed_words_file)\n",
    "            while sys.getsizeof(dictionary) < self.free_memory_available:\n",
    "                # read the next token\n",
    "                token = preprocessed_words.readline()\n",
    "                if not token:\n",
    "                    self.end_preprocessed_words_file = 1\n",
    "                    break\n",
    "                # convert the string to a list of relative_id_song, word and its tf\n",
    "                token = self.strToList(token)\n",
    "                if token[1] not in dictionary:\n",
    "                    dictionary[token[1]] = [(token[0], token[2])]\n",
    "                else:\n",
    "                    dictionary[token[1]].append((token[0], token[2]))\n",
    "            self.current_line_preprocessed_words_file = preprocessed_words.tell()\n",
    "            sorted_terms = sorted(dictionary.keys())\n",
    "            for term in sorted_terms:\n",
    "                block.write(term + \":\" + str(dictionary[term]) + \"\\n\")\n",
    "        self.id_block += 1\n",
    "        \n",
    "        \n",
    "    def create_spimi_blocks(self): \n",
    "        print(\"Creating SPIMI blocks\")\n",
    "        # create a directory to store the blocks\n",
    "        blocks_directory = r\".\\blocks\"\n",
    "        if not os.path.exists(blocks_directory):\n",
    "            os.makedirs(blocks_directory)\n",
    "        \n",
    "            \n",
    "        # read the preprocessed words file as it has the tf for each token in each song, then created size \n",
    "        # limit blocks with spimi algorithm \n",
    "        while(not self.end_preprocessed_words_file):  \n",
    "            self.spimi_invert()\n",
    "       \n",
    "   \n",
    "    \"\"\"\n",
    "    Para el merge se abriran todos los bloques simultaneamente, se leera la primera palabras de cada bloque,\n",
    "    luego usando una pritority queue se obtendra el menor termino lexicografico y se combinaran todas las \n",
    "    listas existentes para dicho termino. Se escribira en un nuevo archivo y se repetira el proceso hasta \n",
    "    que no haya mas.\n",
    "    Dado que se el minHeap se inicializa con 1 termino de cada bloque y cada vez que se hace pop, se pushea \n",
    "    un term del mismo bloque, se garantiza que el heap tiene un term de cada bloque a menos que se haya llegado\n",
    "    al final de un bloque. Ademas, se itera hasta que el heap este vacio, por lo que se garantiza el \n",
    "    recorrido total por todos los bloques.\n",
    "    \"\"\"\n",
    "   \n",
    "    def merge_spimi_blocks(self):\n",
    "        print(\"Merging SPIMI blocks\")\n",
    "        # get all the block names\n",
    "        blocks_name = os.listdir(r\".\\blocks\")\n",
    "        # open all the blocks\n",
    "        opened_blocks = []\n",
    "        for block_name in blocks_name:\n",
    "            opened_blocks.append(open(\".\\\\blocks\\\\\" + block_name, 'r'))\n",
    "        # min heap for terms\n",
    "        terms_from_all_blocks = []\n",
    "        # dictionary to store the norm for each document\n",
    "        temp_norm_dict = {}\n",
    "        # dictionary to store the position of each term in the index file\n",
    "        position_terms = {}\n",
    "        # read the first term from each block\n",
    "        counter_opened_block = 0\n",
    "        for opened_block in opened_blocks:\n",
    "            term = opened_block.readline()\n",
    "            if term:\n",
    "                term = term.split(\":\")\n",
    "                # push (term, [...], block_number) to the heap\n",
    "                heapq.heappush(terms_from_all_blocks, (term[0], term[1], counter_opened_block))\n",
    "                counter_opened_block += 1\n",
    "        # counter of words in the index\n",
    "        counter_words = 0\n",
    "        # merge the blocks in index.txt\n",
    "        with open(self.index, 'w') as index:\n",
    "            prev_top_term = None\n",
    "            temp_prev_top_term = []\n",
    "        \n",
    "            # iterate until the heap is empty\n",
    "            while len(terms_from_all_blocks) > 0:\n",
    "                # obtenemos el menor termino lexicograficamente \n",
    "                current_top_term = heapq.heappop(terms_from_all_blocks)\n",
    "           \n",
    "                \n",
    "                # if the previous term is the same as the current term, merge their posting lists\n",
    "                if prev_top_term:\n",
    "                    \n",
    "                    if prev_top_term[0] == current_top_term[0]:\n",
    "                        # aaaaaaaaaaaaaaaaaaaaaaaaa pythooo\n",
    "                        \n",
    "                        # convert the string to a list of tuples if necessary\n",
    "                        temp_prev_top_term = prev_top_term[1]\n",
    "                        if isinstance(prev_top_term[1], str):                \n",
    "                            temp_prev_top_term = eval(prev_top_term[1])\n",
    "                        # posting list of current term is always a str\n",
    "                        temp_current_top_term = eval(current_top_term[1])\n",
    "                        # merge posting lists\n",
    "                        temp_prev_top_term.extend(temp_current_top_term)\n",
    "                        # update the posting list and block number \n",
    "                        prev_top_term =(prev_top_term[0], temp_prev_top_term, current_top_term[2])\n",
    "                    else:\n",
    "                        # only write the previous term \n",
    "                        if isinstance(prev_top_term[1], str):\n",
    "                            temp_posting_list = eval(prev_top_term[1])\n",
    "                            prev_top_term = (prev_top_term[0], temp_posting_list, prev_top_term[2])\n",
    "                    \n",
    "                        # compute the idf\n",
    "                        idf_t = np.log10(self.cant_documents / len(prev_top_term[1]))\n",
    "                        # compute the tf_idf squared weights of each term for each document, doc is a tuple (doc_id, tf)\n",
    "                        for doc in prev_top_term[1]:\n",
    "                            if doc[0] not in temp_norm_dict:\n",
    "                                temp_norm_dict[doc[0]] = ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                            else:\n",
    "                                temp_norm_dict[doc[0]] += ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                    \n",
    "                        # store the position of the term in the index file\n",
    "                        position_terms[prev_top_term[0]] = index.tell()\n",
    "                        # write the term to the index -> term:idf:posting_list \n",
    "                        index.write(prev_top_term[0] + \":\" + str(idf_t) + \":\" + str(prev_top_term[1]) + \"\\n\")\n",
    "                        counter_words += 1\n",
    "                        if counter_words % 1000 == 0:\n",
    "                            print(\"Words processed:\", counter_words)\n",
    "                        # --------------------------------------------------------- \n",
    "                        # transform to List if posting list is a str\n",
    "                        if isinstance(current_top_term[1], str):\n",
    "                            temp_posting_list = eval(current_top_term[1])\n",
    "                            current_top_term = (current_top_term[0], temp_posting_list, current_top_term[2])\n",
    "                        # update the previous term, not write bc it could be present in other blocks \n",
    "                        prev_top_term = current_top_term\n",
    "                else:\n",
    "                    # 1st term\n",
    "                    prev_top_term = current_top_term\n",
    "                    \n",
    "                # read the next term from the block of the current term\n",
    "                next_term_same_block = opened_blocks[current_top_term[2]].readline()\n",
    "                if next_term_same_block:\n",
    "                       next_term_same_block = next_term_same_block.split(\":\")\n",
    "                       # push the next term to the heap\n",
    "                       heapq.heappush(terms_from_all_blocks, (next_term_same_block[0], next_term_same_block[1], current_top_term[2]))\n",
    "        #--------------------------\n",
    "        # write the last term\n",
    "        if isinstance(prev_top_term[1], str):\n",
    "            temp_posting_list = eval(prev_top_term[1])\n",
    "            prev_top_term = (prev_top_term[0], temp_posting_list, prev_top_term[2])\n",
    "                    \n",
    "            # compute the idf\n",
    "            idf_t = np.log10(self.cant_documents / len(prev_top_term[1]))\n",
    "            # compute the tf_idf squared weights of each term for each document, doc is a tuple (doc_id, tf)\n",
    "            for doc in prev_top_term[1]:\n",
    "                if doc[0] not in temp_norm_dict:\n",
    "                    temp_norm_dict[doc[0]] = ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                else:\n",
    "                    temp_norm_dict[doc[0]] += ((1 + np.log10(doc[1])) * idf_t) ** 2\n",
    "                    \n",
    "                # store the position of the term in the index file\n",
    "                position_terms[prev_top_term[0]] = index.tell()\n",
    "                # write the term to the index -> term:idf:posting_list \n",
    "                index.write(prev_top_term[0] + \":\" + str(idf_t) + \":\" + str(prev_top_term[1]) + \"\\n\")\n",
    "            \n",
    "        \n",
    "        #--------------------------\n",
    "        # compute the norm of each document\n",
    "        for doc in temp_norm_dict:\n",
    "            temp_norm_dict[doc] = np.sqrt(temp_norm_dict[doc])\n",
    "        print(\"Blocks merged\")\n",
    "        # write norms to disk with pickle to recover them later in dictionary format\n",
    "        with open(self.norma_docs_file, 'wb') as file:\n",
    "            pickle.dump(temp_norm_dict, file)\n",
    "        # write the position of each term in the index file to disk with pickle to recover them later\n",
    "        with open(self.position_terms_file, 'wb') as file:\n",
    "            pickle.dump(position_terms, file)\n",
    "        # close blocks\n",
    "        for opened_block in opened_blocks:\n",
    "            opened_block.close()\n",
    "        print(\"Index created\")  \n",
    "    # build the index\n",
    "    def build (self, pathOfFile):\n",
    "        # FIXME: Only build the process if the index file does not exist for development purposes\n",
    "        self.original_file = pathOfFile\n",
    "        self.load_stop_list()\n",
    "        if os.path.exists(self.index):\n",
    "            return\n",
    "        \n",
    "        self.preprocesamiento(pathOfFile)\n",
    "        self.create_spimi_blocks()\n",
    "        self.merge_spimi_blocks()\n",
    "    \n",
    "    def load_index(self):\n",
    "        # load index (position of terms in index file) and the norm of documents from disk       \n",
    "        try:\n",
    "            with open(self.position_terms_file, 'rb') as file:\n",
    "                self.index_main = pickle.load(file)\n",
    "                print(\"Index main ready\")\n",
    "            with open(self.norma_docs_file, 'rb') as file:\n",
    "                self.length_main = pickle.load(file)\n",
    "                print(\"Length main ready\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Index not found\")\n",
    "\n",
    "        \n",
    "    def retrieval(self, query, k):\n",
    "        # load pos terms and norm docs\n",
    "        self.load_index()\n",
    "        # diccionario para el score\n",
    "        score = {}\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        post_query = self.preprocesamiento_aux(query)\n",
    "        # calcular el tf-idf del query\n",
    "        query_words = set(post_query)\n",
    "        query_tfidf = []\n",
    "        with open(self.index, 'r') as idx_file:\n",
    "            for wordq in query_words:\n",
    "                idfq = 0\n",
    "                if wordq in self.index_main:\n",
    "                    # posicion del termino en el index \n",
    "                    pos_wordq = self.index_main[wordq]\n",
    "                    # read the term from the file\n",
    "                    idx_file.seek(pos_wordq)\n",
    "                    term_info = idx_file.readline().split(\":\")\n",
    "                    # idf del termino\n",
    "                    idfq = float(term_info[1])\n",
    "              \n",
    "                  \n",
    "                # tf.idf del termino en el query \n",
    "                query_tfidf.append((1 + np.log10(post_query.count(wordq))) * idfq)\n",
    "        # compute the norm of the query \n",
    "       \n",
    "        query_norm = np.sqrt(sum([q ** 2 for q in query_tfidf]))\n",
    "        # normalizar la query\n",
    "       \n",
    "        query_tfidf = [q / query_norm for q in query_tfidf]\n",
    "        query_words = list(query_words)\n",
    "\n",
    "\n",
    "        temp__ = {}\n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "        with open(self.index, 'r') as idx_file:\n",
    "            for word in query_words:\n",
    "                if word in self.index_main:\n",
    "                    pos_word = self.index_main[word]\n",
    "                    idx_file.seek(pos_word)\n",
    "                    term_info = idx_file.readline().split(\":\")\n",
    "                    idf = float(term_info[1])\n",
    "                    posting_list_ = eval(term_info[2])\n",
    "               \n",
    "                    for doc, tf in posting_list_:\n",
    "                        if doc not in score:\n",
    "                            score[doc] = 0\n",
    "                            temp__[doc] = 0\n",
    "                        # tf.idf del termino en el documento por el tf.idf del termino en el query\n",
    "                        tf_idf_ = (1 + np.log10(tf)) * idf\n",
    "                        score[doc] += query_tfidf[query_words.index(word)] * tf_idf_\n",
    "                        temp__[doc] +=(tf*idf)\n",
    "            \n",
    "           \n",
    "            for d in score:\n",
    "                # normalizar el score\n",
    "                score[d] = score[d] / self.length_main[d]\n",
    "                temp__[d] = temp__[d] / self.length_main[d]\n",
    "        \n",
    "        # transform the dictionary to a list of tuples and sort it by the score \n",
    "        result = sorted(score.items(), key=lambda tup: tup[1], reverse=True) \n",
    "        # return only the keys\n",
    "        # result = [key for key, value in result]\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        print(\"Results retrieved\")\n",
    "        print(\"Top\", k, \"songs:\")\n",
    "        return result[:k]\n",
    "    # FIXME 2: display the results in a dataframe\n",
    "    def displayResults(self, result):\n",
    "       \n",
    "        track_ids = []\n",
    "        track_name = []\n",
    "        artists = []\n",
    "        lyrics = []\n",
    "        scores = []\n",
    "        \n",
    "        # open the file with the songs\n",
    "        songs = pd.read_csv(self.original_file, delimiter=',', quotechar='\"')\n",
    "        for item in result:\n",
    "            # info_new = \"Rank:\" + str(i)+ \"Url :\"+ dataton[\"url\"][resuldato[0]]+ \"score: \"+ str(resuldato[1])\n",
    "            #result_news.append(info_new)\n",
    "            \n",
    "            song = songs.iloc[item[0]- 1] \n",
    "            track_ids.append(song['track_id'])\n",
    "            track_name.append(song['track_name'])\n",
    "            artists.append(song['track_artist'])\n",
    "            lyrics.append(song['lyrics'])\n",
    "            scores.append(str(item[1]))\n",
    "       \n",
    "        df_news = pd.DataFrame({\"ID\": track_ids, \"Track Name\": track_name,\"Artist\": artists ,\"Lyrics\": lyrics, \"Score\": scores})\n",
    "        return df_news\n",
    "\n",
    " # \n"
   ],
   "block_group": "c2e0cdc0e8ff4c99935a31d2e0cfbfb0",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:18:25.314750Z",
     "start_time": "2024-11-18T20:18:25.306524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test preprocess songs\n",
    "index = InvertIndexSPIMI()\n",
    "index.build(r\".\\Postgres\\spotify_songs.csv\")"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:18:30.872648Z",
     "start_time": "2024-11-18T20:18:30.811923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test retrieval\n",
    "# TODO: try gradio, display the results in a dataframe\n",
    "# TODO: integrate with the gui of postgres\n",
    "# TODO: delete comments, clean\n",
    "index.retrieval(\"love\", 5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Results retrieved\n",
      "Top 5 songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5765, np.float64(0.2542631387250887)),\n",
       " (3617, np.float64(0.2351353843279243)),\n",
       " (990, np.float64(0.19833244484991522)),\n",
       " (12170, np.float64(0.19252806846012152)),\n",
       " (6552, np.float64(0.18789486620146836))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:18:32.039569Z",
     "start_time": "2024-11-18T20:18:32.036536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lyric_funky_ride = \"\"\"\n",
    "Lo so che eri abituata a fidarti solo di te stessa Non esisteva nessun'altra regola che questa Ed ogni metro, ogni palmo conquistato è stata una fatica E le discese le puoi contare sulle dita Lo so che eri abituata a ballare solo sulle punte Sopra tutti i chiodi della vita A guardarti le spalle e a difenderti da tutti Anche da chi non ti aveva mai ferita Se avrai torto o ragione per me Non sarà importante Sappi che io sarò sempre Dalla tua parte E senza dubbi ed incertezze Inganni, scuse o debolezze io In ogni giorno, in ogni istante Io sarò dalla tua parte Io sarò dalla tua parte Perché la vita corre in fretta e non c'è tempo di aspettare Che sia tu, la sola terra ferma in tutto questo mare E potrei stare qui in eterno ferma, standoti a guardare Come un continente, un mondo nuovo\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:18:33.032993Z",
     "start_time": "2024-11-18T20:18:32.565769Z"
    }
   },
   "cell_type": "code",
   "source": "index.displayResults(index.retrieval(lyric_funky_ride, 20))\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Results retrieved\n",
      "Top 20 songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                        ID                             Track Name  \\\n",
       "0   0taheMDGubABEnocamFyT5                        Dalla tua parte   \n",
       "1   4uRx3SdWG2cNVfRNVPwHDS          Carillon (with Achille Lauro)   \n",
       "2   09YXhYMYIHZBHVKGEQrSQF                         Return To Love   \n",
       "3   4lCwnBZurX0OIqMKBOHRlS                           Colpo Grosso   \n",
       "4   70RrIIfbY4OjEdX9xnVr3X                                Freedom   \n",
       "5   1hQZiYDGr6bbLc3ZxsT1CM                                   TIME   \n",
       "6   53ROeduQXgL4UgemVlOSKY                       We Like To Party   \n",
       "7   2X0q5QBDUzwLa7O7AwWBzQ                       We Like To Party   \n",
       "8   2ccyIFXCsi1daa3NcaTSLs                          Io Sono Bella   \n",
       "9   7l56nY7NMP1QDdWvRdJmmg  Vivere Tutte Le Vite - con Carl Brave   \n",
       "10  35QutYlXHDefIzlO4GjXjk                         Tua Per Sempre   \n",
       "11  4bC3PgzOLeqOfluhN3nMpf                       Musica che resta   \n",
       "12  2lnzGkdtDj5mtlcOW2yRtG                     Whenever, Wherever   \n",
       "13  1cTIkQUcWtwooj2rDcNB5n                                 My Way   \n",
       "14  2gSVKxPDww9Eep5rdvtdem                                 My Way   \n",
       "15  1L8kJLp3ExF5ZGHwEZEqHe                                   Tush   \n",
       "16  4I72mMJwJ3jnnRls1SBxws                                   Tush   \n",
       "17  37eGbhE1xVFSvcKkqGb6i1                        Contra La Pared   \n",
       "18  6e0CvGZf7CouOpYF8toXHC                             Body On My   \n",
       "19  05R2lE5g9hnaSmxIrUl3C0                        I Adore Mi Amor   \n",
       "\n",
       "                Artist                                             Lyrics  \\\n",
       "0   Alessandra Amoroso  Lo so che eri abituata a fidarti solo di te st...   \n",
       "1               Nahaze  Ohh-ohh, ah Wide awake, can't think straight I...   \n",
       "2       Andrea Bocelli  Quand'è che spento il cuore? Che ho smesso di ...   \n",
       "3                 Snik  NA Δε θέλω χρόνο μαζί σου Ήρθα να δω το χαρτί ...   \n",
       "4             Zucchero  Mi svegliai, tentennai, me ne andai, yeah Birr...   \n",
       "5         Mondo Grosso  部屋の窓辺で 夜をほどいた 君の瞳は まだ夕暮れの波 肌に染まった 香りあやして 手と手繋げ...   \n",
       "6              Showtek  La-di-da-di La-di-da-di La-di-da-di, we like t...   \n",
       "7              Showtek  La-di-da-di La-di-da-di La-di-da-di, we like t...   \n",
       "8                 Emma  Fammi godere adesso Solo per un istante Io mi ...   \n",
       "9                Elisa  Stesi alla luce del giorno Con gli occhi anneg...   \n",
       "10               Elisa  Amore mio ti aspetto sempre E mai un'attesa è ...   \n",
       "11             Il Volo  Leggo in fondo ai tuoi pensieri Cerco in un so...   \n",
       "12             Shakira  Lucky you were born that far away so We could ...   \n",
       "13         Limp Bizkit  Check, check, check, check, check, che-eck, ch...   \n",
       "14         Limp Bizkit  Check, check, check, check, check, che-eck, ch...   \n",
       "15              ZZ Top  I been up, I been down Take my word, my way ar...   \n",
       "16              ZZ Top  I been up, I been down Take my word, my way ar...   \n",
       "17           Sean Paul  NA SP 'longside J Balvin, men El negocio socio...   \n",
       "18         Loud Luxury  Jaja, everybody knows I'm live (Candela) Every...   \n",
       "19       Color Me Badd  Dream on, dream away I think I'm gonna have to...   \n",
       "\n",
       "                   Score  \n",
       "0    0.25512478205005745  \n",
       "1    0.14319373939710214  \n",
       "2    0.14115478158298975  \n",
       "3    0.13813681770400646  \n",
       "4    0.11161477051438247  \n",
       "5     0.0975894753876482  \n",
       "6    0.09594601698284995  \n",
       "7    0.09594601698284995  \n",
       "8    0.06737979232268926  \n",
       "9    0.06470582580438927  \n",
       "10   0.06467301216089512  \n",
       "11   0.06266470300845547  \n",
       "12    0.0605314435984062  \n",
       "13   0.05698502455908609  \n",
       "14   0.05698502455908609  \n",
       "15   0.05432944116483079  \n",
       "16   0.05432944116483079  \n",
       "17  0.052542791172523036  \n",
       "18   0.05170586337487275  \n",
       "19  0.051639594376319875  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0taheMDGubABEnocamFyT5</td>\n",
       "      <td>Dalla tua parte</td>\n",
       "      <td>Alessandra Amoroso</td>\n",
       "      <td>Lo so che eri abituata a fidarti solo di te st...</td>\n",
       "      <td>0.25512478205005745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4uRx3SdWG2cNVfRNVPwHDS</td>\n",
       "      <td>Carillon (with Achille Lauro)</td>\n",
       "      <td>Nahaze</td>\n",
       "      <td>Ohh-ohh, ah Wide awake, can't think straight I...</td>\n",
       "      <td>0.14319373939710214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09YXhYMYIHZBHVKGEQrSQF</td>\n",
       "      <td>Return To Love</td>\n",
       "      <td>Andrea Bocelli</td>\n",
       "      <td>Quand'è che spento il cuore? Che ho smesso di ...</td>\n",
       "      <td>0.14115478158298975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4lCwnBZurX0OIqMKBOHRlS</td>\n",
       "      <td>Colpo Grosso</td>\n",
       "      <td>Snik</td>\n",
       "      <td>NA Δε θέλω χρόνο μαζί σου Ήρθα να δω το χαρτί ...</td>\n",
       "      <td>0.13813681770400646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70RrIIfbY4OjEdX9xnVr3X</td>\n",
       "      <td>Freedom</td>\n",
       "      <td>Zucchero</td>\n",
       "      <td>Mi svegliai, tentennai, me ne andai, yeah Birr...</td>\n",
       "      <td>0.11161477051438247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1hQZiYDGr6bbLc3ZxsT1CM</td>\n",
       "      <td>TIME</td>\n",
       "      <td>Mondo Grosso</td>\n",
       "      <td>部屋の窓辺で 夜をほどいた 君の瞳は まだ夕暮れの波 肌に染まった 香りあやして 手と手繋げ...</td>\n",
       "      <td>0.0975894753876482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53ROeduQXgL4UgemVlOSKY</td>\n",
       "      <td>We Like To Party</td>\n",
       "      <td>Showtek</td>\n",
       "      <td>La-di-da-di La-di-da-di La-di-da-di, we like t...</td>\n",
       "      <td>0.09594601698284995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2X0q5QBDUzwLa7O7AwWBzQ</td>\n",
       "      <td>We Like To Party</td>\n",
       "      <td>Showtek</td>\n",
       "      <td>La-di-da-di La-di-da-di La-di-da-di, we like t...</td>\n",
       "      <td>0.09594601698284995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2ccyIFXCsi1daa3NcaTSLs</td>\n",
       "      <td>Io Sono Bella</td>\n",
       "      <td>Emma</td>\n",
       "      <td>Fammi godere adesso Solo per un istante Io mi ...</td>\n",
       "      <td>0.06737979232268926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7l56nY7NMP1QDdWvRdJmmg</td>\n",
       "      <td>Vivere Tutte Le Vite - con Carl Brave</td>\n",
       "      <td>Elisa</td>\n",
       "      <td>Stesi alla luce del giorno Con gli occhi anneg...</td>\n",
       "      <td>0.06470582580438927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35QutYlXHDefIzlO4GjXjk</td>\n",
       "      <td>Tua Per Sempre</td>\n",
       "      <td>Elisa</td>\n",
       "      <td>Amore mio ti aspetto sempre E mai un'attesa è ...</td>\n",
       "      <td>0.06467301216089512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4bC3PgzOLeqOfluhN3nMpf</td>\n",
       "      <td>Musica che resta</td>\n",
       "      <td>Il Volo</td>\n",
       "      <td>Leggo in fondo ai tuoi pensieri Cerco in un so...</td>\n",
       "      <td>0.06266470300845547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2lnzGkdtDj5mtlcOW2yRtG</td>\n",
       "      <td>Whenever, Wherever</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>Lucky you were born that far away so We could ...</td>\n",
       "      <td>0.0605314435984062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1cTIkQUcWtwooj2rDcNB5n</td>\n",
       "      <td>My Way</td>\n",
       "      <td>Limp Bizkit</td>\n",
       "      <td>Check, check, check, check, check, che-eck, ch...</td>\n",
       "      <td>0.05698502455908609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2gSVKxPDww9Eep5rdvtdem</td>\n",
       "      <td>My Way</td>\n",
       "      <td>Limp Bizkit</td>\n",
       "      <td>Check, check, check, check, check, che-eck, ch...</td>\n",
       "      <td>0.05698502455908609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1L8kJLp3ExF5ZGHwEZEqHe</td>\n",
       "      <td>Tush</td>\n",
       "      <td>ZZ Top</td>\n",
       "      <td>I been up, I been down Take my word, my way ar...</td>\n",
       "      <td>0.05432944116483079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4I72mMJwJ3jnnRls1SBxws</td>\n",
       "      <td>Tush</td>\n",
       "      <td>ZZ Top</td>\n",
       "      <td>I been up, I been down Take my word, my way ar...</td>\n",
       "      <td>0.05432944116483079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37eGbhE1xVFSvcKkqGb6i1</td>\n",
       "      <td>Contra La Pared</td>\n",
       "      <td>Sean Paul</td>\n",
       "      <td>NA SP 'longside J Balvin, men El negocio socio...</td>\n",
       "      <td>0.052542791172523036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6e0CvGZf7CouOpYF8toXHC</td>\n",
       "      <td>Body On My</td>\n",
       "      <td>Loud Luxury</td>\n",
       "      <td>Jaja, everybody knows I'm live (Candela) Every...</td>\n",
       "      <td>0.05170586337487275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>05R2lE5g9hnaSmxIrUl3C0</td>\n",
       "      <td>I Adore Mi Amor</td>\n",
       "      <td>Color Me Badd</td>\n",
       "      <td>Dream on, dream away I think I'm gonna have to...</td>\n",
       "      <td>0.051639594376319875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Postgres"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:30.781230Z",
     "start_time": "2024-11-18T20:30:30.270500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Guardamos en un dataframe la data del archivo csv\n",
    "csv_file_path = f\"./Postgres/spotify_songs.csv\"\n",
    "# Cargar el CSV en un DataFrame de pandas\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Mostrar las primeras filas del archivo\n",
    "print(df.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id                                         track_name  \\\n",
      "0  0017A6SJgTbfQVU2EtsPNo                                           Pangarap   \n",
      "1  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
      "2  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
      "3  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
      "4  00emjlCv9azBN0fzuuyLqy                                         Dumb Litty   \n",
      "\n",
      "      track_artist                                             lyrics  \\\n",
      "0  Barbie's Cradle  Minsan pa Nang ako'y napalingon Hindi ko alam ...   \n",
      "1    Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
      "2   Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
      "3      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
      "4             KARD  Get up out of my business You don't keep me fr...   \n",
      "\n",
      "   track_popularity          track_album_id  \\\n",
      "0                41  1srJQ0njEQgd8w4XSqI4JQ   \n",
      "1                28  3z04Lb9Dsilqw68SHt6jLB   \n",
      "2                 0  6oZ6brjB8x3GoeSYdwJdPc   \n",
      "3                41  3ssspRe42CXkhPxdc12xcp   \n",
      "4                65  7h5X3xhh3peIK9Y0qI5hbK   \n",
      "\n",
      "                       track_album_name track_album_release_date  \\\n",
      "0                                  Trip               2001-01-01   \n",
      "1                           Love & Loss               2017-11-21   \n",
      "2                                  Gold               2005-01-01   \n",
      "3                  CeeLo's Magic Moment               2012-10-29   \n",
      "4  KARD 2nd Digital Single ‘Dumb Litty’               2019-09-22   \n",
      "\n",
      "                                       playlist_name             playlist_id  \\\n",
      "0                                 Pinoy Classic Rock  37i9dQZF1DWYDQ8wBxd7xt   \n",
      "1                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
      "2  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
      "3                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
      "4                                  K-Party Dance Mix  37i9dQZF1DX4RDXswvP6Mj   \n",
      "\n",
      "   ... loudness mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
      "0  ...  -10.068    1       0.0236       0.27900           0.01170    0.0887   \n",
      "1  ...   -4.739    1       0.0442       0.01170           0.00994    0.3470   \n",
      "2  ...   -7.504    0       0.2160       0.00432           0.00723    0.4890   \n",
      "3  ...   -5.819    0       0.0341       0.68900           0.00000    0.0664   \n",
      "4  ...   -1.993    1       0.0409       0.03700           0.00000    0.1380   \n",
      "\n",
      "   valence    tempo  duration_ms  language  \n",
      "0    0.566   97.091       235440        tl  \n",
      "1    0.404  135.225       373512        en  \n",
      "2    0.650  111.904       262467        en  \n",
      "3    0.405  118.593       243067        en  \n",
      "4    0.240  130.018       193160        en  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:35.877982Z",
     "start_time": "2024-11-18T20:30:33.503399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Hacemos la coneccion con PostgreSQL\n",
    "import sqlalchemy as sa\n",
    "print(sa.__version__)\n",
    "import pandas as pd\n",
    "\n",
    "# Crea la conexión a la base de datos\n",
    "# engine = sa.create_engine('postgresql://postgres:1234@localhost:5432/Data')#NO OLVIDAR cambiar el nombre de la base de datos y la contraseña\n",
    "engine = sa.create_engine('postgresql://postgres:postgres@localhost:5432/data')#\n",
    "# Prueba la conexión\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Conexión exitosa a la base de datos.\")\n",
    "except Exception as e:\n",
    "    print(\"Error en la conexión:\", e)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.36\n",
      "Conexión exitosa a la base de datos.\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:39.558439Z",
     "start_time": "2024-11-18T20:30:37.013007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Subimos el DataFrame como una tabla en PostgreSQL\n",
    "nombre_tabla = \"Canciones\"\n",
    "df.to_sql(nombre_tabla, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(f\"La tabla '{nombre_tabla}' ha sido creada en PostgreSQL.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla 'Canciones' ha sido creada en PostgreSQL.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:42.035111Z",
     "start_time": "2024-11-18T20:30:41.997461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Consulta de prueba\n",
    "# Consulta de ejemplo: selecciona todas las filas de la tabla \"Canciones\"\n",
    "query = ''' CREATE EXTENSION IF NOT EXISTS pg_trgm;\n",
    "            SELECT track_name FROM public.\"Canciones\" LIMIT 10;'''\n",
    "df_query = pd.read_sql_query(query, engine)\n",
    "print(df_query)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          track_name\n",
      "0                                           Pangarap\n",
      "1                                       I Feel Alive\n",
      "2                                             Poison\n",
      "3  Baby It's Cold Outside (feat. Christina Aguilera)\n",
      "4                                         Dumb Litty\n",
      "5                                            Soldier\n",
      "6                                          Lakshmana\n",
      "7                                        Satisfy You\n",
      "8                                       Tender Lover\n",
      "9                      Hide Away (feat. Envy Monroe)\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:43.245455Z",
     "start_time": "2024-11-18T20:30:43.242142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sqlalchemy import text\n",
    "def execute_query(query):\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(query))\n",
    "        connection.commit()\n"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:44.790120Z",
     "start_time": "2024-11-18T20:30:44.168311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Concatenar las columnas track_name, track_artist, lyrics\n",
    "query='''ALTER TABLE public.\"Canciones\" ADD COLUMN full_text TEXT;\n",
    "UPDATE public.\"Canciones\"\n",
    "SET full_text= CONCAT(track_name,' ',track_artist,' ',lyrics);'''\n",
    "execute_query(query)\n"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:30:52.519560Z",
     "start_time": "2024-11-18T20:30:47.545508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Crear un atributo de tipo vector textual\n",
    "query = ''' alter table public.\"Canciones\" add column lyrics_w tsvector; \n",
    "            update public.\"Canciones\" set lyrics_w = to_tsvector(\"full_text\");\n",
    "            '''\n",
    "execute_query(query)\n"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:31:12.197651Z",
     "start_time": "2024-11-18T20:31:11.313121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Crear un atributo de tipo vector textual\n",
    "# agregar el indice\n",
    "query = '''CREATE INDEX lyrics_w_idx ON public.\"Canciones\" USING gin (lyrics_w);'''\n",
    "execute_query(query)\n"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:31:17.122442Z",
     "start_time": "2024-11-18T20:31:17.117425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "query = '''set enable_seqscan = off;'''\n",
    "execute_query(query)\n"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:31:18.597334Z",
     "start_time": "2024-11-18T20:31:18.547394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 3. Aplicar las consultas\n",
    "query='''\n",
    "        SELECT track_name, track_artist, lyrics, ts_rank_cd(lyrics_w, query) as score\n",
    "        FROM public.\"Canciones\", to_tsquery('hello | love ') query\n",
    "        WHERE query @@ lyrics_w\n",
    "        order by score DESC\n",
    "        limit 10;\n",
    "      '''\n",
    "df_query = pd.read_sql_query(query, engine)\n",
    "print(df_query)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          track_name      track_artist  \\\n",
      "0                                Lose You To Love Me      Selena Gomez   \n",
      "1  How You Love Me (feat. Conor Maynard & Snoop D...          Hardwell   \n",
      "2                                In the Name of Love     Martin Garrix   \n",
      "3                                  Give Me Your Love            Sigala   \n",
      "4                                Lose You To Love Me      Selena Gomez   \n",
      "5                                              SUGAR      BROCKHAMPTON   \n",
      "6                             Remember (with ZOHARA)           Gryffin   \n",
      "7                                    Who Do You Love  The Chainsmokers   \n",
      "8                                       Shape of You        Ed Sheeran   \n",
      "9                                      Love No Limit     Mary J. Blige   \n",
      "\n",
      "                                              lyrics      score  \n",
      "0  You promised the world and I fell for it I put...  25.500000  \n",
      "1  Big Snoop Dogg My nephew Hardwell on the beat ...  18.700000  \n",
      "2  If I told you this was only gonna hurt If I wa...  16.100000  \n",
      "3  Your love, love your love Your love, love your...  15.400001  \n",
      "4  You promised the world and I fell for it I put...  14.500000  \n",
      "5  Spendin' all my nights alone, waitin' for you ...  14.400001  \n",
      "6  You love to talk without thinking But never kn...  13.600000  \n",
      "7  Yeah Found cigarettes in your Fendi coat Even ...  12.700000  \n",
      "8  The club isn't the best place to find a lover ...  12.500000  \n",
      "9  Baby, there's no need to tell you As far as I ...  12.300000  \n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parser : PostgreSQL"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:35:22.572828Z",
     "start_time": "2024-11-18T20:35:22.567081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import re\n",
    "def consultasql(text):\n",
    "\n",
    "\n",
    "    with open(r\"./stoplists/en.txt\", encoding=\"utf-8\") as file:\n",
    "        stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "    columnas= r\"select\\s+([^;]*)\\s+from\"  # me da lo que esta entre 'select' y 'from'\n",
    "    palabras = r\"liketo\\s+'(.*?)'\"  # obtiene la o las palabras que estan en comillas\n",
    "    limite= r\"limit\\s+(\\d+)\"  # obtiene el limite\n",
    "    columnaspre = re.search(columnas, text, re.IGNORECASE)\n",
    "    palabraspre = re.search(palabras, text, re.IGNORECASE)\n",
    "    limitepre= re.search(limite, text, re.IGNORECASE)\n",
    "    if columnaspre!=None:\n",
    "        columnasfinal = columnaspre.group(1).strip()\n",
    "    else:\n",
    "        columnasfinal =''\n",
    "    if palabraspre!=None:\n",
    "        palabrasfinal = palabraspre.group(1)\n",
    "    else:\n",
    "        palabrasfinal=''\n",
    "    if limitepre!=None:\n",
    "        limitefinal = limitepre.group(0)\n",
    "    else:\n",
    "        limitefinal=''\n",
    "\n",
    "    palabrasq = palabrasfinal.split()# divido mis palabras\n",
    "    palabrasq = [palabra for palabra in palabrasq if palabra.lower() not in stoplist]# elimino lo que no me da mucha información\n",
    "    palabrasya =\" | \".join(palabrasq)\n",
    "\n",
    "    tsqueri = f\"to_tsquery('{palabrasya}')\"\n",
    "\n",
    "    #aqui esta el procedimiento de la tranformacion del texto a una query para que lo acepte sql y me retorne el resultado\n",
    "    query='''SELECT '''+columnasfinal+''', ts_rank(lyrics_w, query) as score FROM public.\"Canciones\",''' +tsqueri +'''query\n",
    "    WHERE query @@ lyrics_w\n",
    "    order by score DESC '''+limitefinal+''' ;'''\n",
    "\n",
    "    df_query = pd.read_sql_query(query, engine)\n",
    "    return df_query\n"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:37:56.097638Z",
     "start_time": "2024-11-18T20:37:56.067062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test with a italian song\n",
    "query ='''select track_name,track_artist,lyrics from Canciones where lyric liketo 'Lo so che eri abituata a fidarti solo di te stessa Non esisteva nessun altra regola che questa Ed ogni metro, ogni palmo conquistato è stata una fatica' limit 10;'''\n",
    "consultasql(query)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                     track_name        track_artist  \\\n",
       "0                               Dalla tua parte  Alessandra Amoroso   \n",
       "1        MAI (feat. Lele Blade & Fred De Palma)              Giaime   \n",
       "2       Senza Di Me (feat. Venerus & Franco126)            Gemitaiz   \n",
       "3                            Sale (feat. Shari)        Benji & Fede   \n",
       "4  Ti volevo dedicare (feat. J-AX & Boomdabash)          Rocco Hunt   \n",
       "5                                         Lento           Boro Boro   \n",
       "6                                   Porto Cervo               Lazza   \n",
       "7              La ragazza con il cuore di latta               Irama   \n",
       "8             pers0na2 - feat. Gemitaiz, Madman         tha Supreme   \n",
       "9                       La Tua Futura Ex Moglie       Willie Peyote   \n",
       "\n",
       "                                              lyrics     score  \n",
       "0  Lo so che eri abituata a fidarti solo di te st...  0.069971  \n",
       "1  Vamos a bailar esta vida nueva Non mi dici mai...  0.034843  \n",
       "2  Ciao, amore Non piangere, ti mando un bacio (m...  0.033995  \n",
       "3  Come i fiori finti non profumano Tutte le fras...  0.032874  \n",
       "4  Ho una cosa da dirti da tempo Ma  non ho mai t...  0.032121  \n",
       "5  Ehi, ehi Baby, sto fuman- Ehi D-D-D-Don Joe Ba...  0.031056  \n",
       "6  Yah, yah, yah, yah La la la la la333 Mob La la...  0.031032  \n",
       "7  Fare l'amore è così facile credo Amare una per...  0.030456  \n",
       "8  Yah, yah, yah, ehi, yeah, yeah, yeah, (yeah), ...  0.030371  \n",
       "9  Portami via da qua con te, potrà sembrarti pre...  0.030357  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dalla tua parte</td>\n",
       "      <td>Alessandra Amoroso</td>\n",
       "      <td>Lo so che eri abituata a fidarti solo di te st...</td>\n",
       "      <td>0.069971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAI (feat. Lele Blade &amp; Fred De Palma)</td>\n",
       "      <td>Giaime</td>\n",
       "      <td>Vamos a bailar esta vida nueva Non mi dici mai...</td>\n",
       "      <td>0.034843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senza Di Me (feat. Venerus &amp; Franco126)</td>\n",
       "      <td>Gemitaiz</td>\n",
       "      <td>Ciao, amore Non piangere, ti mando un bacio (m...</td>\n",
       "      <td>0.033995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sale (feat. Shari)</td>\n",
       "      <td>Benji &amp; Fede</td>\n",
       "      <td>Come i fiori finti non profumano Tutte le fras...</td>\n",
       "      <td>0.032874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ti volevo dedicare (feat. J-AX &amp; Boomdabash)</td>\n",
       "      <td>Rocco Hunt</td>\n",
       "      <td>Ho una cosa da dirti da tempo Ma  non ho mai t...</td>\n",
       "      <td>0.032121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lento</td>\n",
       "      <td>Boro Boro</td>\n",
       "      <td>Ehi, ehi Baby, sto fuman- Ehi D-D-D-Don Joe Ba...</td>\n",
       "      <td>0.031056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Porto Cervo</td>\n",
       "      <td>Lazza</td>\n",
       "      <td>Yah, yah, yah, yah La la la la la333 Mob La la...</td>\n",
       "      <td>0.031032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>La ragazza con il cuore di latta</td>\n",
       "      <td>Irama</td>\n",
       "      <td>Fare l'amore è così facile credo Amare una per...</td>\n",
       "      <td>0.030456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pers0na2 - feat. Gemitaiz, Madman</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>Yah, yah, yah, ehi, yeah, yeah, yeah, (yeah), ...</td>\n",
       "      <td>0.030371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>La Tua Futura Ex Moglie</td>\n",
       "      <td>Willie Peyote</td>\n",
       "      <td>Portami via da qua con te, potrà sembrarti pre...</td>\n",
       "      <td>0.030357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2- Interfaz Gráfica"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:38:07.504248Z",
     "start_time": "2024-11-18T20:38:07.499151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: refactor this function\n",
    "# Wrapper function to use the SPIMI algorithm\n",
    "def wrapper(query, k):\n",
    "    result = index.retrieval(query, k)\n",
    "    return index.displayResults(result)"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:38:17.235133Z",
     "start_time": "2024-11-18T20:38:16.477449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the wrapper function\n",
    "results_wrapper = wrapper(\"I really can't stay Baby it's cold outside I've got to go away Baby it's cold out there This evening has been Been hoping that you'd drop in So very nice I'll hold your hands, they're just like ice My mother will start to worry Beautiful, what's your hurry? My father will be pacing the floor Listen to that fireplace roar So really I'd better scurry Beautiful, please don't hurry Well maybe just a half a drink more Why don't you put some records on while I pour The neighbors might think Baby, it's bad out there Say, what's in this drink? No cabs to be had out there I wish I knew how Your eyes are like starlight To break the spell I'll take your hat, your hair looks swell I ought to say no, no, no, sir Mind if I move in closer? At least I'm gonna say that I tried What's the sense in hurting my pride? I really can't stay Baby don't hold out Baby it's cold outside I simply must go See that it's cold outside The answer is no I said it's cold out there This welcome has been How lucky that you dropped in So nice and warm Look out the window at that storm My sister will be suspicious Gosh, your lips look delicious My brother will be there at the door Waves upon a tropical shore My maiden aunt's mind is vicious Oh, your lips are delicious Maybe just a cigarette more Never such a blizzard before Hey I've got to go home Baby, you'll freeze out there Say, lend me your coat It's up to your knees out there You've really been grand I'm thrilled when you touch my hand But don't you see How can you do this thing to me? There's bound to be talk tomorrow Think of my life long sorrow At least there will be plenty implied If you caught pneumonia and died I really can't stay Get over that old lie Baby, baby it's cold outside\", 5)\n",
    "print(results_wrapper)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index main ready\n",
      "Length main ready\n",
      "Results retrieved\n",
      "Top 5 songs:\n",
      "                       ID                                         Track Name  \\\n",
      "0  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
      "1  3Nx6iu7XpzWL1QMPRJKzqL                             Baby It's Cold Outside   \n",
      "2  4PQUmnZUbTRz72oAtk3lkn                                         By My Side   \n",
      "3  7qEKqBCD2vE5vIBsrUitpD                                            ilomilo   \n",
      "4  3yUcJwYu7fXAfqMj9krY6l                                          Thank You   \n",
      "\n",
      "            Artist                                             Lyrics  \\\n",
      "0      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
      "1  Ella Fitzgerald  I really can't stay (But baby it's cold outsid...   \n",
      "2           CALVIN  Lay your hand in mine and watch the starlight ...   \n",
      "3    Billie Eilish  Told you not to worry But maybe that's a lie H...   \n",
      "4             Dido  My tea's gone cold, I'm wondering why I got ou...   \n",
      "\n",
      "                 Score  \n",
      "0   0.9449541433377157  \n",
      "1    0.910231778702112  \n",
      "2   0.1383622230144945  \n",
      "3  0.12038283745470736  \n",
      "4  0.11199641881991683  \n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:39:08.717165Z",
     "start_time": "2024-11-18T20:39:03.161895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import gradio as gr\n",
    "# FIXME: arreglar warnings, hacer mas bonita la interfaz y mas explicativa. Inegrar con la interfaz de la parte 3\n",
    "def interface_1():\n",
    "    \"\"\"\n",
    "    Primera interfaz para el algoritmo SPIMI.\n",
    "    \"\"\"\n",
    "    with gr.Blocks(theme='HaleyCH/HaleyCH_Theme') as demo1:\n",
    "    \n",
    "        demo1.title = \"SPIMI Algorithm\"\n",
    "\n",
    "  \n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Ingrese su consulta aquí...\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            k_input = gr.Slider(\n",
    "                label=\"Top Scores\",\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                value=5,\n",
    "                step=1\n",
    "            )\n",
    "\n",
    "   \n",
    "        with gr.Row():\n",
    "            output_df = gr.DataFrame(\n",
    "                headers=None,\n",
    "                datatype=\"str\",\n",
    "                label=\"Resultados\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\n",
    "                \"Get Results\",\n",
    "                variant=\"primary\"\n",
    "            )\n",
    "\n",
    "   \n",
    "        submit_btn.click(\n",
    "            fn=wrapper,\n",
    "            inputs=[query_input, k_input],\n",
    "            outputs=output_df\n",
    "        )\n",
    "\n",
    "    return demo1\n",
    "\n",
    "def interface_2():\n",
    "    \"\"\"\n",
    "    Segunda interfaz para el índice GIN.\n",
    "    \"\"\"\n",
    "    with gr.Blocks(theme='HaleyCH/HaleyCH_Theme') as demo2:\n",
    "      \n",
    "        demo2.title = \"GIN Index\"\n",
    "\n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Ingrese su consulta aquí...\"\n",
    "            )\n",
    "\n",
    "       \n",
    "        with gr.Row():\n",
    "            output_df = gr.DataFrame(\n",
    "                headers=None,\n",
    "                datatype=\"str\",\n",
    "                label=\"Resultados\"\n",
    "            )\n",
    "\n",
    "    \n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\n",
    "                \"Get Results\",\n",
    "                variant=\"primary\"\n",
    "            )\n",
    "\n",
    "   \n",
    "        submit_btn.click(\n",
    "            fn=lambda query: consultasql(query),\n",
    "            inputs=[query_input],\n",
    "            outputs=output_df\n",
    "        )\n",
    "\n",
    "    return demo2\n",
    "\n",
    "def main_demo():\n",
    "    \"\"\"\n",
    "    Función principal que crea el layout con ambas interfaces\n",
    "    \"\"\"\n",
    "    with gr.Blocks(theme='HaleyCH/HaleyCH_Theme') as main_demo:\n",
    "        gr.Markdown(\"Spotify Songs Search Engine\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                demo1 = interface_1()\n",
    "                \n",
    "            with gr.Column():\n",
    "                demo2 = interface_2()\n",
    "               \n",
    "\n",
    "    main_demo.launch()\n",
    "    return main_demo\n",
    "\n",
    "main_interfaz = main_demo()\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Code\\UTEC\\BD2\\Proyecto2\\Proyecto_DB2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T20:43:32.517430Z",
     "start_time": "2024-11-18T20:43:32.376964Z"
    }
   },
   "cell_type": "code",
   "source": "main_interfaz.close()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:55:14.542799Z",
     "start_time": "2024-10-24T09:55:10.846102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=wrapper,\n",
    "    inputs=[gr.Text(label=\"Query\"), gr.Slider(label=\"Top Scores\")],\n",
    "    outputs=[gr.Dataframe(\n",
    "        headers=[\"ID\", \"Url\", \"Score\"],\n",
    "        datatype=[\"str\", \"str\", \"str\"],\n",
    "    )],\n",
    "    title=\"Proyecto DB2\",\n",
    "    examples=[\n",
    "        [\"El pais de China y su cooperacion\", 5],\n",
    "        [\"Salud y bienestar en la sociedad\", 10],\n",
    "    ],\n",
    "    theme='HaleyCH/HaleyCH_Theme'\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 19
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_persisted_session": {
   "createdAt": "2024-05-23T14:46:04.322Z"
  },
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_notebook_id": "0f6751b6349d4dc18cbbb391b3b45a06",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
