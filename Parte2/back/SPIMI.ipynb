{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f72575afc9b54cf98a7b62a31329ed15",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "id": "fr2ewyCfSkO1"
   },
   "source": [
    "# Backend"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a16f9b93bc0d4ae1984d870600e37c22",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1731,
    "execution_start": 1716474219828,
    "id": "yVHsYSHpUwrb",
    "outputId": "2f7d71c6-2dfb-4f87-db53-a763f7c26a15",
    "source_hash": null,
    "ExecuteTime": {
     "end_time": "2024-11-12T05:51:48.965513Z",
     "start_time": "2024-11-12T05:51:47.111855Z"
    }
   },
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "183e8ad18357406c839500c461ab9be0",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "id": "XPoEHpwva7eP"
   },
   "source": [
    "### 1. Preparación\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T06:06:14.504691Z",
     "start_time": "2024-11-12T06:06:14.216674Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "# abrir el csv\n",
    "list = [] # Guarda las canciones\n",
    "nombres = [] # Nombres de las canciones\n",
    "i = 0\n",
    "with open(r'..\\Postgres\\spotify_songs.csv', newline='', encoding='utf-8') as file:\n",
    "    songs_file = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "    for row in songs_file:\n",
    "        # track_name , artist_name, lyrics\n",
    "        song_details = row[1] + \" \" + row[2] + \" \" + row[3]  \n",
    "        nombres.append(row[1])\n",
    "        list.append(song_details)\n",
    " \n",
    "  \n",
    "\n",
    "list.pop(0)\n",
    "nombres.pop(0)\n",
    "\n",
    "print(list[0])\n",
    "print(nombres[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pangarap Barbie's Cradle Minsan pa Nang ako'y napalingon Hindi ko alam Na ika'y tutugon Sa mga tanong na aking nabitawan Hindi ko alam kung ito'y totoo Pangarap ka Sa bawat sandali Langit man ang tingin ko Sayo sana'y marating Hanggang dito na lang yata Ang kaya kong gawin Mangarap na lang At bumulong sa hangin Kailan kaya Darating ulit ang isang Sandali Na ako'y lilingon muli Pangarap ka o tinig mong kay lamig Ang iyong mga ngiti na sa akin ay Nakapagbigay pansin (Ikaw ba ay isang pangarap lang) Pangarap ka o tinig mong kay lamig Ang iyong mga ngiti Na sa akin ay Nakapagbigay... Pangarap ka o tinig mong kay lamig Ang iyong mga ngiti Na sa akin ay Nakapagbigay Pangarap ka o tinig mong kay lamig Ang iyong mga ngiti Na sa akin ay Nakapagbigay pansin\n",
      "Pangarap\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nombres:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "76f388ea5e764423b0898bcdd3b4199d",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### 1- Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "ec9b2746d7b14b9bb3d599053a4ecde6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1377,
    "execution_start": 1716474221565,
    "id": "X31vxQmca_nu",
    "outputId": "68f6a6ec-6366-410d-b1a7-ff4616cab1fc",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Tokenizar, filtrar stopwords, eliminar signos, stemming\n",
    "\n",
    "with open(\"stoplist.txt\", encoding=\"latin-1\") as file:\n",
    "    stoplist = [line.rstrip().lower() for line in file]\n",
    "    \n",
    "def preprocesamiento(texto,stemming=True):\n",
    "  words = []\n",
    "  # 1- convertir a minusculas\n",
    "  texto = texto.lower()\n",
    "  # 2- eliminar signos con regex\n",
    "  texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "  # 3- tokenizar\n",
    "  words = nltk.word_tokenize(texto, language='english')\n",
    "  # 3- eliminar stopwords\n",
    "  words = [word for word in words if word not in stoplist]\n",
    "  # 4- Aplicar reduccion de palabras (stemming)\n",
    "  if stemming:        \n",
    "      words = [stemmer.stem(word) for word in words]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar las canciones\n",
    "textos_procesados = [] # lista de listas de palabras de cada cancion\n",
    "# indice = {}\n",
    "\n",
    "for i in range(0, len(list)):\n",
    "  texto = preprocesamiento(list[i])\n",
    "  textos_procesados.append(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(textos_procesados)):\n",
    "  print(textos_procesados[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "88673c7c872e45a89a3844c73b31e8b8",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "id": "dXBNvBLxTnTy"
   },
   "source": [
    "### 2- Construcción del índice TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(collection):\n",
    "    # Calcular los pesos TF-IDF para cada documento de la colección\n",
    "    N = len(collection)\n",
    "    \n",
    "    # Listar todos los términos en la coleccion\n",
    "    terms = set()\n",
    "    for doc in collection:\n",
    "        for word in doc:\n",
    "            terms.add(word)\n",
    "    # print(1)\n",
    "\n",
    "    # Calcular la frecuencia de palabras en cada documento usando el set de todas las palabras (terms)\n",
    "    # tf\n",
    "    freqs = [] \n",
    "    for doc in collection:\n",
    "        freq = {}\n",
    "        for word in doc:\n",
    "            if word in terms:\n",
    "                freq[word] = freq.get(word, 0) + 1\n",
    "        freqs.append(freq)\n",
    "    # print(2)\n",
    "\n",
    "    # Calcular el IDF para cada término\n",
    "    idfs = {word: 0 for word in terms}\n",
    "    for doc in collection:\n",
    "        unique_words_in_doc = set(doc)\n",
    "        for word in unique_words_in_doc:\n",
    "            idfs[word] += 1\n",
    "    # print(3)\n",
    "\n",
    "    # Calcular IDF para todos los términos\n",
    "    for word in idfs:\n",
    "        idfs[word] = np.log(N / idfs[word])\n",
    "    # print(4)\n",
    "\n",
    "    # Calcular los pesos TF-IDF para cada documento\n",
    "    tfidf = []\n",
    "    for i, doc in enumerate(collection):\n",
    "        doc_tfidf = {}\n",
    "        for word in freqs[i]:  # Solo calcular TF-IDF para términos presentes en el documento\n",
    "            tf = freqs[i][word] # cantidad de frecuancia por palabra\n",
    "            doc_tfidf[word] = tf * idfs[word]\n",
    "        tfidf.append(doc_tfidf)\n",
    "\n",
    "    # print(5)\n",
    "    return tfidf, freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "ad3c009f68734c60b8e21f88547d116a",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 615,
    "execution_start": 1716474222946,
    "id": "k4uL88ZGbiCf",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "textos_tfidf, tf_list = compute_tfidf(textos_procesados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir los tf\n",
    "for i in tf_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir los pesos de los términos\n",
    "# for i in range(0, len(textos_tfidf)):\n",
    "#   print(textos_tfidf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardarlo en memoria secundaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construcción del índice en memoria secundaria (SPIMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diccionario1 = {\"1\":[\"doc1\"], \"3\":[\"doc1\", \"doc3\"]}\n",
    "# diccionario2 = {\"2\":[\"doc1\", \"doc2\", \"doc3\"]}\n",
    "\n",
    "# block_size = 3\n",
    "# output = {\"1\":[\"doc1\"], \"2\":[\"doc1\", \"doc2\"]}\n",
    "# output = {\"2\":[\"doc3\"], \"3\":[\"doc1\", \"doc3\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "1319.0\n"
     ]
    }
   ],
   "source": [
    "filename = \"data.dat\"\n",
    "textos_procesados_size = len(textos_procesados)\n",
    "\n",
    "dictionary_num = round(np.log2(textos_procesados_size)) # numero de diccionario generado\n",
    "print(dictionary_num)\n",
    "docs_num = np.ceil(textos_procesados_size/dictionary_num)\n",
    "print(docs_num)\n",
    "\n",
    "\n",
    "\n",
    "bucket_count = 0 # cantidad de buckets\n",
    "\n",
    "block_size = 1 * 2**20 # MB\n",
    "# print(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bucket:\n",
    "    def __init__(self, lista = {}, next=None):\n",
    "        self.lista = lista # {doc: tf} # 5 bytes\n",
    "        self.next = next  # Siguiente bloque (memoria) # 4 bytes\n",
    "\n",
    "    def Write(self):\n",
    "        with open(filename, 'ab') as f:\n",
    "            p = f.tell()\n",
    "            pickle.dump(self, f)\n",
    "            return p\n",
    "\n",
    "    @staticmethod\n",
    "    def Read(bucket):\n",
    "        with open(filename, 'rb') as f:\n",
    "            f.seek(bucket)\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    def add_doc(self, doc, tf): # añadir un doc y tf\n",
    "        dictionary = {doc: tf}\n",
    "        self.lista[doc] = dictionary\n",
    "\n",
    "    def insert_next_bucket(self):\n",
    "        self.next = Bucket()\n",
    "    \n",
    "    def next_bucket(self, next): # siguiente bucket\n",
    "        self.next = next\n",
    "        return self.next\n",
    "\n",
    "    def bucket_size(self): # tamaño del bucket\n",
    "        serializado = pickle.dumps(self.lista) + pickle.dumps(self.next)\n",
    "        return len(serializado)\n",
    "\n",
    "    def get_list(self): # get list\n",
    "        return self.lista\n",
    "\n",
    "    def sort_bucket(self):\n",
    "        self.lista = dict(sorted(self.lista.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self, terminos={}): # {word: [df, bucket]}\n",
    "        self.terminos = terminos\n",
    "\n",
    "    def Write(self):\n",
    "        # with open(filename, 'ab') as f:\n",
    "        #     self.bucket = f.tell()    # Marca el lugar de escritura en el archivo\n",
    "        #     pickle.dump(self, f)\n",
    "        bucket = Bucket()\n",
    "        for termino in self.terminos:\n",
    "            # ingresar a los buckets\n",
    "            bucket = self.terminos[termino][1]\n",
    "            \n",
    "            while (bucket != None):\n",
    "                bucket.Write()\n",
    "                bucket = bucket.next\n",
    "\n",
    "    @staticmethod\n",
    "    def Read(bucket):\n",
    "        with open(filename, 'rb') as f:\n",
    "            f.seek(bucket)\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    def insert_term(self, termino): # inserta un termino\n",
    "        if termino not in self.terminos: # si no esta en el diccionario\n",
    "            self.terminos[termino] = [1, Bucket()] # añadir un bucket\n",
    "        else: # si esta en el diccionario\n",
    "            self.incrementar_df(termino) # incrementa el df\n",
    "            \n",
    "\n",
    "\n",
    "    def insert_doc(self, termino, doc, tf, bucket):\n",
    "        if bucket.bucket_size() < block_size: # si el tamaño del bucket es menor al limite\n",
    "            bucket.add_doc(doc, tf) # añadir documento en el bucket\n",
    "        else: # ir al siguiente bucket\n",
    "            if (bucket.next == None): # si no hay siguiente bucket\n",
    "                bucket.insert_next_bucket() # crear siguiente bucket\n",
    "            else: # si hay siguiente bucket\n",
    "                self.insert_doc(self, termino, doc, tf, self.terminos[termino][1].next)\n",
    "\n",
    "\n",
    "\n",
    "    def incrementar_df(self, termino):\n",
    "        self.terminos[termino][0] += 1\n",
    "        \n",
    "    # def insert_bucket(self, termino, doc, tf):\n",
    "    #     self.terminos[termino][1] = Bucket()\n",
    "\n",
    "    def sort_dictionary(self): # ordenar los terminos\n",
    "        self.terminos = dict(sorted(self.terminos.items(), key=lambda item: item[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeBlocks(fn):\n",
    "    for i in range(0, len(fn)):\n",
    "        # Merge blocks\n",
    "        # leer dos bloques de memoria secundaria\n",
    "        input1 = fn[i].Read(filename, i)\n",
    "        input2 = fn[j].Read(filename, j)\n",
    "        output = {}\n",
    "        \n",
    "        # merge\n",
    "        for j in range(0, int(np.log2(n))): # cantidad de niveles\n",
    "            for k in range(0, int(n/2)): # cantidad de bloques\n",
    "                output[input1.keys()[k]] = input1.values()[k] + input2.values()[k]\n",
    "        # \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPIMI_Invert(docs, doc_num): # terminos de un documento\n",
    "    dictionary = Dictionary() # Dictionary / (word, df, bucket) / diccionario local / guarda el enlace a la cabecera de lista de bloques de publicaciones (posting_list)\n",
    "    # posting_list = Bucket()  # Bucket / lista de publicaciones / lista de {doci, tf}\n",
    "    pos = 0\n",
    "    \n",
    "    # while (len(terminos)>pos): # mientras hay terminos, se llena el diccionario\n",
    "    for doc in docs:\n",
    "        for termino in doc: # terminos del doc i\n",
    "            pos+=1\n",
    "\n",
    "            dictionary.insert_term(termino) # añadir un termino al diccionario\n",
    "\n",
    "            dictionary.insert_doc(termino, nombres[doc_num], tf_list[doc_num][termino], dictionary.terminos[termino][1]) # añadir el documento\n",
    "            \n",
    "            # posting_list = dictionary.terminos[termino][1] # obtener el Bucket del termino\n",
    "\n",
    "            # if (dictionary.terminos[termino][1].bucket_size() > block_size): # si el tamaño del bloque es mayor al tamaño del bloque\n",
    "            #     dictionary.next_bucket(termino) # añadir un bucket\n",
    "            \n",
    "            # docID = nombres[doc_num]\n",
    "            \n",
    "        doc_num += 1\n",
    "\n",
    "    # Ordenar el diccionario\n",
    "    dictionary.sort_dictionary()\n",
    "\n",
    "    # Escribir los buckets en memoria secundaria / el diccionario en RAM\n",
    "    dictionary.Write()\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BSBIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18454\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# print(len(textos_procesados))\n",
    "# print(round(np.log2(len(textos_procesados))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = [] # lista de diccionarios locales / indices invertidos locales (key, df, bucket)\n",
    "\n",
    "\n",
    "def BSBIndex():\n",
    "    docs = []\n",
    "    i = 0\n",
    "    doc_num = 0 # que documento se está procesando\n",
    "\n",
    "    for doc in textos_procesados: # juntar canciones\n",
    "        docs.append(doc)\n",
    "        i+=1\n",
    "\n",
    "        if i >= docs_num or i == textos_procesados_size: \n",
    "            fn.append(SPIMI_Invert(docs, doc_num))\n",
    "            docs = []\n",
    "        \n",
    "\n",
    "    mergeBlocks(fn) # todos los diccionarios/indices locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'i'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mBSBIndex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[26], line 14\u001B[0m, in \u001B[0;36mBSBIndex\u001B[1;34m()\u001B[0m\n\u001B[0;32m     11\u001B[0m     i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m docs_num \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m textos_procesados_size: \n\u001B[1;32m---> 14\u001B[0m         fn\u001B[38;5;241m.\u001B[39mappend(\u001B[43mSPIMI_Invert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoc_num\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     15\u001B[0m         docs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     18\u001B[0m mergeBlocks(fn)\n",
      "Cell \u001B[1;32mIn[25], line 13\u001B[0m, in \u001B[0;36mSPIMI_Invert\u001B[1;34m(docs, doc_num)\u001B[0m\n\u001B[0;32m      9\u001B[0m     pos\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     11\u001B[0m     dictionary\u001B[38;5;241m.\u001B[39minsert_term(termino) \u001B[38;5;66;03m# añadir un termino al diccionario\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m     dictionary\u001B[38;5;241m.\u001B[39minsert_doc(termino, nombres[doc_num], \u001B[43mtf_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdoc_num\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtermino\u001B[49m\u001B[43m]\u001B[49m, dictionary\u001B[38;5;241m.\u001B[39mterminos[termino][\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;66;03m# añadir el documento\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# posting_list = dictionary.terminos[termino][1] # obtener el Bucket del termino\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m# if (dictionary.terminos[termino][1].bucket_size() > block_size): # si el tamaño del bloque es mayor al tamaño del bloque\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m#     dictionary.next_bucket(termino) # añadir un bucket\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     \n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# docID = nombres[doc_num]\u001B[39;00m\n\u001B[0;32m     22\u001B[0m doc_num \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'i'"
     ]
    }
   ],
   "source": [
    "BSBIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(Q, Doc):\n",
    "    # Obtener los términos comunes entre los dos documentos\n",
    "    terms_comun = set(Q.keys()).intersection(set(Doc.keys()))\n",
    "    \n",
    "    # Calcular el producto punto\n",
    "    dot_product = sum(Q[term] * Doc[term] for term in terms_comun)\n",
    "    \n",
    "    # Calcular las normas (longitudes) de los vectores\n",
    "    norm_Q = np.sqrt(sum(value**2 for value in Q.values()))\n",
    "    norm_Doc = np.sqrt(sum(value**2 for value in Doc.values()))\n",
    "    \n",
    "    # Evitar división por cero\n",
    "    if norm_Q == 0 or norm_Doc == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Retornar la similitud de coseno\n",
    "    return dot_product / (norm_Q * norm_Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similitud de coseno en memoria secundaria\n",
    "# q: texto de consulta\n",
    "# min_sim: similitud minima\n",
    "def cosineScore(q, min_sim=0.8, filename, N):\n",
    "    float Scores[N]=0\n",
    "    float Length[N]\n",
    "    float result = []\n",
    "    for i in range(0, N): # itera por cada una de las paginas\n",
    "        posting_list = read(filename, i) # toma una pagina i del disco\n",
    "        for t in q: # itera los terminos de la query y lo compara con los de la posting_list\n",
    "            calculate_wtq(posting_list, t)\n",
    "            for d, tf in posting_list:\n",
    "                Scores[d] += wtd*wtq\n",
    "\n",
    "    Length = size(posting_list) # asigna las longitudes de cada documento\n",
    "    for d in posting_list: # itera por los documentos de la posting_list\n",
    "        Scores[d] = Scores[d]/Length[d]\n",
    "        if Scores[d] > min_sim: # inserta los documentos que cumplen con ser mayores al min_sim\n",
    "            result.append(d)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "0f6751b6349d4dc18cbbb391b3b45a06",
  "deepnote_persisted_session": {
   "createdAt": "2024-05-23T14:46:04.322Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
